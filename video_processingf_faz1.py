# -*- coding: utf-8 -*-
"""video-processingf-faz1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gFJmANoS5nxTNW7ST8kmAN5z-8tO6-4d

Ã¶nce bu model iÃ§in saniyesaniye playerlarÄ±n-referee+goalkeeperlarÄ±n konumlarÄ±nÄ±n Ã§Ä±ktÄ±sÄ±nÄ± al
sonra modeli gerekirse eÄŸitirsin. bi Ã¶nceÃ§Ä±ktÄ± al. bunun iÃ§in classlÄ± cellde kalmÄ±ÅŸtÄ±n. onu run ettirip. sonuÃ§larÄ± pitche gÃ¶re ya da boxa gÃ¶re(eÄŸer box pitche gÃ¶reyse bir de soccernetteki ball neye gÃ¶re konum belirtiyorsa ona gÃ¶re Ã§Ä±ktÄ±yÄ± kaydet.)

## Video Processing

## key ayarlarÄ±

### Configure your API keys
    - `HF_TOKEN`.
    - `ROBOFLOW_API_KEY`.
"""

!nvidia-smi

"""## BaÄŸÄ±mlÄ±lÄ±klar"""

!pip install -q gdown inference-gpu
!pip install -q onnxruntime-gpu==1.18.0 --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/

"""roboflow sports"""

!pip install -q git+https://github.com/roboflow/sports.git

!pip uninstall -y supervision && pip install -q supervision>=0.23.0

import os
os.environ["ONNXRUNTIME_EXECUTION_PROVIDERS"] = "[CUDAExecutionProvider]"

"""## ball, player, goalkeeper and referee detection"""

!pip install numpy==1.25

from inference import get_model
from google.colab import userdata

ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')
PLAYER_DETECTION_MODEL_ID = "football-players-detection-3zvbc/12"
PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)

from google.colab import drive
drive.mount('/content/drive')

import supervision as sv

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_11_fraport-tav-antalyaspor_besiktas.mp4"

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

sv.plot_image(frame)

import cv2
import numpy as np
import matplotlib.pyplot as plt
import random

# Video dosyasÄ±nÄ± aÃ§
cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)

# Toplam frame sayÄ±sÄ±nÄ± al
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Rastgele 5 frame seÃ§
random_frames = sorted(random.sample(range(total_frames), 5))

print("SeÃ§ilen frame numaralarÄ±:", random_frames)

# SeÃ§ilen frame'leri gÃ¶ster
plt.figure(figsize=(15, 5))

for i, frame_number in enumerate(random_frames):
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)  # Belirtilen frame'e git
    ret, frame = cap.read()  # Frame'i oku

    if ret:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # OpenCV BGR formatÄ±nÄ± RGB'ye Ã§evir
        plt.subplot(1, 5, i+1)  # 5 gÃ¶rselli bir subplot
        plt.imshow(frame)
        plt.title(f"Frame {frame_number}")
        plt.axis("off")
    else:
        print(f"{frame_number} numaralÄ± frame alÄ±namadÄ±.")

cap.release()
plt.show()

import supervision as sv

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"

box_annotator = sv.BoxAnnotator(
    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),
    text_color=sv.Color.from_hex('#000000')
)

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
detections = sv.Detections.from_inference(result)

labels = [
    f"{class_name} {confidence:.2f}"
    for class_name, confidence
    in zip(detections['class_name'], detections.confidence)
]

annotated_frame = frame.copy()
annotated_frame = box_annotator.annotate(
    scene=annotated_frame,
    detections=detections)
annotated_frame = label_annotator.annotate(
    scene=annotated_frame,
    detections=detections,
    labels=labels)

sv.plot_image(annotated_frame)

import supervision as sv
import random
import cv2
import matplotlib.pyplot as plt

# Video dosyasÄ±nÄ± belirle
SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"

# Anotasyonlar iÃ§in renkler ve stiller
box_annotator = sv.BoxAnnotator(
    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),
    text_color=sv.Color.from_hex('#000000')
)

# Videoyu aÃ§ ve toplam frame sayÄ±sÄ±nÄ± Ã¶ÄŸren
cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Rastgele 5 farklÄ± frame seÃ§
random_frames = sorted(random.sample(range(total_frames), 5))
print("SeÃ§ilen frame numaralarÄ±:", random_frames)

# SeÃ§ilen frame'leri iÅŸle ve gÃ¶ster
plt.figure(figsize=(15, 10))

for i, frame_number in enumerate(random_frames):
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)  # Belirtilen frame'e git
    ret, frame = cap.read()  # Frame'i oku

    if ret:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # OpenCV'den gelen frame'i RGB'ye Ã§evir

        # Oyuncu tespiti modeli ile frame Ã¼zerinde inference yap
        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        detections = sv.Detections.from_inference(result)

        # Etiketleri oluÅŸtur
        labels = [
            f"{class_name} {confidence:.2f}"
            for class_name, confidence
            in zip(detections['class_name'], detections.confidence)
        ]

        # AnotasyonlarÄ± uygula
        annotated_frame = frame.copy()
        annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)
        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)

        # GÃ¶rÃ¼ntÃ¼yÃ¼ subplot iÃ§inde gÃ¶ster
        plt.subplot(2, 3, i+1)  # 2 satÄ±r, 3 sÃ¼tunluk grid ÅŸeklinde dÃ¼zenler
        plt.imshow(annotated_frame)
        plt.title(f"Frame {frame_number}")
        plt.axis("off")
    else:
        print(f"{frame_number} numaralÄ± frame alÄ±namadÄ±.")

cap.release()
plt.tight_layout()
plt.show()

"""## video game style visualization"""

import supervision as sv

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
BALL_ID = 0

ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=25,
    height=21,
    outline_thickness=1
)

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
detections = sv.Detections.from_inference(result)

ball_detections = detections[detections.class_id == BALL_ID]
ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

all_detections = detections[detections.class_id != BALL_ID]
all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
all_detections.class_id -= 1

annotated_frame = frame.copy()
annotated_frame = ellipse_annotator.annotate(
    scene=annotated_frame,
    detections=all_detections)
annotated_frame = triangle_annotator.annotate(
    scene=annotated_frame,
    detections=ball_detections)

sv.plot_image(annotated_frame)

import supervision as sv
import cv2
import matplotlib.pyplot as plt

# Ball ID
BALL_ID = 0

# Anotasyon tanÄ±mlarÄ±
ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=25,
    height=21,
    outline_thickness=1
)

# Videoyu tekrar aÃ§
cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)

# SeÃ§ilen frame'leri iÅŸle ve gÃ¶ster
plt.figure(figsize=(15, 10))

for i, frame_number in enumerate(random_frames):  # Ã–nceki hÃ¼crede seÃ§ilen frame'leri kullanÄ±yoruz
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)  # Belirtilen frame'e git
    ret, frame = cap.read()  # Frame'i oku

    if ret:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # OpenCV'den gelen frame'i RGB'ye Ã§evir

        # Oyuncu tespiti modeli ile frame Ã¼zerinde inference yap
        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        detections = sv.Detections.from_inference(result)

        # Top ve diÄŸer nesneleri ayÄ±r
        ball_detections = detections[detections.class_id == BALL_ID]
        ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)  # Top iÃ§in kutuyu geniÅŸlet

        all_detections = detections[detections.class_id != BALL_ID]
        all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
        all_detections.class_id -= 1  # ID'leri yeniden dÃ¼zenle

        # AnotasyonlarÄ± uygula
        annotated_frame = frame.copy()
        annotated_frame = ellipse_annotator.annotate(scene=annotated_frame, detections=all_detections)
        annotated_frame = triangle_annotator.annotate(scene=annotated_frame, detections=ball_detections)

        # GÃ¶rÃ¼ntÃ¼yÃ¼ subplot iÃ§inde gÃ¶ster
        plt.subplot(2, 3, i+1)  # 2 satÄ±r, 3 sÃ¼tun dÃ¼zeninde gÃ¶rÃ¼ntÃ¼leri yerleÅŸtirir
        plt.imshow(annotated_frame)
        plt.title(f"Frame {frame_number}")
        plt.axis("off")
    else:
        print(f"{frame_number} numaralÄ± frame alÄ±namadÄ±.")

cap.release()
plt.tight_layout()
plt.show()

"""## player tracking"""

import supervision as sv

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
BALL_ID = 0

ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    text_color=sv.Color.from_hex('#000000'),
    text_position=sv.Position.BOTTOM_CENTER
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=25,
    height=21,
    outline_thickness=1
)

tracker = sv.ByteTrack()
tracker.reset()

# AÃ§mak istediÄŸiniz spesifik frame numarasÄ±
FRAME_NUMBER = 1200

# OpenCV ile videoyu aÃ§
cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)

# Ä°lgili frame numarasÄ±na git
cap.set(cv2.CAP_PROP_POS_FRAMES, FRAME_NUMBER)

# Frame'i oku
ret, frame = cap.read()

# Frame baÅŸarÄ±yla alÄ±ndÄ±ysa devam et
if ret:
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # OpenCV BGR formatÄ±nÄ± RGB'ye Ã§evir
    sv.plot_image(frame)  # Supervision ile frame'i gÃ¶ster
else:
    print(f"{FRAME_NUMBER}. frame alÄ±namadÄ±. Video dosyasÄ± eksik veya bozuk olabilir.")

result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
detections = sv.Detections.from_inference(result)

ball_detections = detections[detections.class_id == BALL_ID]
ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

all_detections = detections[detections.class_id != BALL_ID]
all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
all_detections.class_id -= 1
all_detections = tracker.update_with_detections(detections=all_detections)

labels = [
    f"#{tracker_id}"
    for tracker_id
    in all_detections.tracker_id
]

annotated_frame = frame.copy()
annotated_frame = ellipse_annotator.annotate(
    scene=annotated_frame,
    detections=all_detections)
annotated_frame = label_annotator.annotate(
    scene=annotated_frame,
    detections=all_detections,
    labels=labels)
annotated_frame = triangle_annotator.annotate(
    scene=annotated_frame,
    detections=ball_detections)

sv.plot_image(annotated_frame)

import supervision as sv
import random
import cv2
import matplotlib.pyplot as plt

# Video dosyasÄ±
SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
BALL_ID = 0  # Topun sÄ±nÄ±f ID'si

# Anotasyon tanÄ±mlarÄ±
ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    text_color=sv.Color.from_hex('#000000'),
    text_position=sv.Position.BOTTOM_CENTER
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=25,
    height=21,
    outline_thickness=1
)

# Tracker baÅŸlat
tracker = sv.ByteTrack()
tracker.reset()

# Videoyu aÃ§ ve toplam frame sayÄ±sÄ±nÄ± Ã¶ÄŸren
cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Daha Ã¶nce seÃ§ilmiÅŸ random frame numaralarÄ± (Ã–nceki hÃ¼crede tanÄ±mlanmÄ±ÅŸ olmalÄ±)
print("Ã–nceki hÃ¼crede seÃ§ilen frame numaralarÄ±:", random_frames)

# SeÃ§ilen frame'leri iÅŸle ve gÃ¶ster
plt.figure(figsize=(15, 10))

for i, frame_number in enumerate(random_frames):  # Ã–nceki hÃ¼crede belirlenen random frame'ler
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)  # Belirtilen frame'e git
    ret, frame = cap.read()  # Frame'i oku

    if ret:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # OpenCV formatÄ±nÄ± RGB'ye Ã§evir

        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        detections = sv.Detections.from_inference(result)

        ball_detections = detections[detections.class_id == BALL_ID]
        ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

        all_detections = detections[detections.class_id != BALL_ID]
        all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
        all_detections.class_id -= 1
        all_detections = tracker.update_with_detections(detections=all_detections)

        labels = [
            f"#{tracker_id}"
            for tracker_id
            in all_detections.tracker_id
        ]

        annotated_frame = frame.copy()
        annotated_frame = ellipse_annotator.annotate(
            scene=annotated_frame,
            detections=all_detections)
        annotated_frame = label_annotator.annotate(
            scene=annotated_frame,
            detections=all_detections,
            labels=labels)
        annotated_frame = triangle_annotator.annotate(
            scene=annotated_frame,
            detections=ball_detections)

        sv.plot_image(annotated_frame)

cap.release()
plt.tight_layout()
plt.show()

import supervision as sv
import cv2

# Video dosyanÄ±z
SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
BALL_ID = 0

# Ã‡Ä±ktÄ± videosunun kaydedileceÄŸi dosya
OUTPUT_VIDEO_PATH = "/content/tracker_output_5min.mp4"

# VideoCapture ile videoyu aÃ§
cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)

# Video Ã¶zelliklerini al
fps = int(cap.get(cv2.CAP_PROP_FPS))  # FPS deÄŸerini al
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Videodaki toplam frame sayÄ±sÄ±

# Ä°lk 5 dakika iÃ§in frame sÄ±nÄ±rÄ±
max_frames = min(fps * 60 * 2, total_frames)  # 5 dakika * 60 saniye * FPS

# OpenCV VideoWriter ile Ã§Ä±ktÄ±yÄ± kaydetme
fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 formatÄ± iÃ§in codec
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))

# Tracker baÅŸlat
tracker = sv.ByteTrack()
tracker.reset()

# Anotasyon tanÄ±mlarÄ±
ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    text_color=sv.Color.from_hex('#000000'),
    text_position=sv.Position.BOTTOM_CENTER
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=25,
    height=21,
    outline_thickness=1
)

# Frame'leri sÄ±rayla iÅŸle (Ä°lk 5 dakika iÃ§in)
frame_count = 0
while cap.isOpened() and frame_count < max_frames:
    ret, frame = cap.read()
    if not ret:
        break  # Video bitti

    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # OpenCV frame formatÄ±nÄ± RGB'ye Ã§evir

    # Nesne tespiti modeli ile frame Ã¼zerinde iÅŸlem yap
    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)

    # Top ve diÄŸer nesneleri ayÄ±r
    ball_detections = detections[detections.class_id == BALL_ID]
    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

    all_detections = detections[detections.class_id != BALL_ID]
    all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
    all_detections.class_id -= 1
    all_detections = tracker.update_with_detections(detections=all_detections)

    # Tracker ID etiketlerini oluÅŸtur
    labels = [
        f"#{tracker_id}"
        for tracker_id in all_detections.tracker_id
    ]

    # AnotasyonlarÄ± uygula
    annotated_frame = frame.copy()
    annotated_frame = ellipse_annotator.annotate(scene=annotated_frame, detections=all_detections)
    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=all_detections, labels=labels)
    annotated_frame = triangle_annotator.annotate(scene=annotated_frame, detections=ball_detections)

    # Frame'i RGB'den BGR'ye Ã§evir (OpenCV iÃ§in)
    annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)

    # Frame'i video dosyasÄ±na kaydet
    out.write(annotated_frame)

    frame_count += 1  # Frame sayÄ±sÄ±nÄ± artÄ±r

# KaynaklarÄ± serbest bÄ±rak
cap.release()
out.release()
cv2.destroyAllWindows()

print(f"Ä°lk 5 dakikanÄ±n iÅŸlendiÄŸi video kaydedildi: {OUTPUT_VIDEO_PATH}")

"""OBJE DETECTION + CLUSTERING + NUMBER RECOGNITION and endly TRACKING

## split players into teams

![football AI diagram](https://media.roboflow.com/notebooks/examples/football-ai-team-clustering.png)
"""

from tqdm import tqdm

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
PLAYER_ID = 2
STRIDE = 30

frame_generator = sv.get_video_frames_generator(
    source_path=SOURCE_VIDEO_PATH, stride=STRIDE)

crops = []
for frame in tqdm(frame_generator, desc='collecting crops'):
    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)
    detections = detections.with_nms(threshold=0.5, class_agnostic=True)
    detections = detections[detections.class_id == PLAYER_ID]
    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]
    crops += players_crops

"""**Note:** croplar"""

sv.plot_images_grid(crops[:100], grid_size=(10, 10))

"""**Note:**  [SigLIP](https://huggingface.co/docs/transformers/en/model_doc/siglip)"""

import torch
from transformers import AutoProcessor, SiglipVisionModel

SIGLIP_MODEL_PATH = 'google/siglip-base-patch16-224'

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
EMBEDDINGS_MODEL = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)
EMBEDDINGS_PROCESSOR = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)

import numpy as np
from more_itertools import chunked

BATCH_SIZE = 32

crops = [sv.cv2_to_pillow(crop) for crop in crops]
batches = chunked(crops, BATCH_SIZE)
data = []
with torch.no_grad():
    for batch in tqdm(batches, desc='embedding extraction'):
        inputs = EMBEDDINGS_PROCESSOR(images=batch, return_tensors="pt").to(DEVICE)
        outputs = EMBEDDINGS_MODEL(**inputs)
        embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()
        data.append(embeddings)

data = np.concatenate(data)

"""**Note:** Using [UMAP](https://github.com/lmcinnes/umap), we project our embeddings from `(N, 768)` to `(N, 3)` and then perform a two-cluster division using [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."""

import umap
from sklearn.cluster import KMeans

REDUCER = umap.UMAP(n_components=3)
CLUSTERING_MODEL = KMeans(n_clusters=2)

projections = REDUCER.fit_transform(data)
clusters = CLUSTERING_MODEL.fit_predict(projections)

import plotly.graph_objects as go
import numpy as np
from typing import Dict, List
from IPython.core.display import display, HTML
from PIL import Image
import base64
from io import BytesIO


def pil_image_to_data_uri(image: Image.Image) -> str:
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    return f"data:image/png;base64,{img_str}"


def display_projections(
    labels: np.ndarray,
    projections: np.ndarray,
    images: List[Image.Image],
    show_legend: bool = False,
    show_markers_with_text: bool = True
) -> None:
    image_data_uris = {f"image_{i}": pil_image_to_data_uri(image) for i, image in enumerate(images)}
    image_ids = np.array([f"image_{i}" for i in range(len(images))])

    unique_labels = np.unique(labels)
    traces = []
    for unique_label in unique_labels:
        mask = labels == unique_label
        customdata_masked = image_ids[mask]
        trace = go.Scatter3d(
            x=projections[mask][:, 0],
            y=projections[mask][:, 1],
            z=projections[mask][:, 2],
            mode='markers+text' if show_markers_with_text else 'markers',
            text=labels[mask],
            customdata=customdata_masked,
            name=str(unique_label),
            marker=dict(size=8),
            hovertemplate="<b>class: %{text}</b><br>image ID: %{customdata}<extra></extra>"
        )
        traces.append(trace)

    fig = go.Figure(data=traces)
    fig.update_layout(
        scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),
        width=1000,
        height=1000,
        showlegend=show_legend
    )

    plotly_div = fig.to_html(full_html=False, include_plotlyjs=False, div_id="scatter-plot-3d")

    javascript_code = f"""
    <script>
        function displayImage(imageId) {{
            var imageElement = document.getElementById('image-display');
            var placeholderText = document.getElementById('placeholder-text');
            var imageDataURIs = {image_data_uris};
            imageElement.src = imageDataURIs[imageId];
            imageElement.style.display = 'block';
            placeholderText.style.display = 'none';
        }}

        var chartElement = document.getElementById('scatter-plot-3d');

        chartElement.on('plotly_click', function(data) {{
            var customdata = data.points[0].customdata;
            displayImage(customdata);
        }});
    </script>
    """

    html_template = f"""
    <!DOCTYPE html>
    <html>
        <head>
            <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
            <style>
                #image-container {{
                    position: fixed;
                    top: 0;
                    left: 0;
                    width: 200px;
                    height: 200px;
                    padding: 5px;
                    border: 1px solid #ccc;
                    background-color: white;
                    z-index: 1000;
                    box-sizing: border-box;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    text-align: center;
                }}
                #image-display {{
                    width: 100%;
                    height: 100%;
                    object-fit: contain;
                }}
            </style>
        </head>
        <body>
            {plotly_div}
            <div id="image-container">
                <img id="image-display" src="" alt="Selected image" style="display: none;" />
                <p id="placeholder-text">Click on a data entry to display an image</p>
            </div>
            {javascript_code}
        </body>
    </html>
    """

    display(HTML(html_template))

display_projections(clusters, projections, crops)

"""**Note:**SigLIP, UMAP ve KMeans combo"""

import supervision as sv
from tqdm import tqdm
from sports.common.team import TeamClassifier

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
PLAYER_ID = 2
STRIDE = 30

frame_generator = sv.get_video_frames_generator(
    source_path=SOURCE_VIDEO_PATH, stride=STRIDE)

crops = []
for frame in tqdm(frame_generator, desc='collecting crops'):
    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)
    players_detections = detections[detections.class_id == PLAYER_ID]
    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]
    crops += players_crops

team_classifier = TeamClassifier(device="cuda")
team_classifier.fit(crops)

"""**Note:** kaleci en yakÄ±n 3 oyuncu takÄ±mÄ±na gÃ¶re takÄ±m atanÄ±r"""

import numpy as np
import supervision as sv

def resolve_goalkeepers_team_id(
    players: sv.Detections,
    goalkeepers: sv.Detections
) -> np.ndarray:
    goalkeepers_xy = goalkeepers.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
    players_xy = players.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
    team_0_centroid = players_xy[players.class_id == 0].mean(axis=0)
    team_1_centroid = players_xy[players.class_id == 1].mean(axis=0)
    goalkeepers_team_id = []
    for goalkeeper_xy in goalkeepers_xy:
        dist_0 = np.linalg.norm(goalkeeper_xy - team_0_centroid)
        dist_1 = np.linalg.norm(goalkeeper_xy - team_1_centroid)
        goalkeepers_team_id.append(0 if dist_0 < dist_1 else 1)

    return np.array(goalkeepers_team_id)

"""#### Tracking with jersey number

OCR BaÅŸlatÄ±cÄ± ve Mapping SÃ¶zlÃ¼ÄŸÃ¼ (setup)
"""

import json
import os
import cv2
from collections import defaultdict, Counter

ocr_reader = easyocr.Reader(['en'], gpu=True)
os.makedirs("debug_crops", exist_ok=True)

# Tracker ID â†’ Forma numarasÄ± eÅŸleÅŸmeleri
tracker_id_to_jersey_number = {}

# ArdÄ±ÅŸÄ±k OCR sonuÃ§larÄ±: tracker_id â†’ liste
ocr_results_buffer = defaultdict(list)

# Daha Ã¶nce kaydedilen forma numaralarÄ± varsa yÃ¼kle
JERSEY_MAPPING_PATH = "jersey_mapping.json"
if os.path.exists(JERSEY_MAPPING_PATH):
    with open(JERSEY_MAPPING_PATH, "r") as f:
        tracker_id_to_jersey_number = {
            int(k): int(v) for k, v in json.load(f).items()
        }

# Ãœst vÃ¼cut bÃ¶lgesini croplayan fonksiyon
def crop_upper_body(xyxy, frame, expand_px=30):
    x1, y1, x2, y2 = map(int, xyxy)
    width = x2 - x1
    height = y2 - y1

    # TÃ¼m Ã¼st gÃ¶vdeyi alma â€” Ã¼st + bir miktar bel kÄ±smÄ±
    y2_new = y1 + int(height * 0.65)

    x1 = max(0, x1 - expand_px)
    y1 = max(0, y1 - expand_px)
    x2 = min(frame.shape[1], x2 + expand_px)
    y2_new = min(frame.shape[0], y2_new + expand_px)

    return frame[y1:y2_new, x1:x2]

# OCR ile forma numarasÄ± okuma
def get_jersey_number_from_crop(crop_image):
    results = ocr_reader.readtext(crop_image, allowlist='0123456789')
    for bbox, text, confidence in results:
        text_clean = text.strip().replace(" ", "")
        if text_clean.isdigit():
            number = int(text_clean)
            if 0 < number <= 99:
                return number
    return None

def save_crop_with_label(crop, path, label):
    import cv2
    font = cv2.FONT_HERSHEY_SIMPLEX
    labeled = crop.copy()
    cv2.putText(labeled, label, (5, 15), font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
    cv2.imwrite(path, labeled)

def preprocess_for_ocr(image):
    import cv2
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Kontrast ve keskinlik artÄ±rma
    sharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    sharpened = cv2.filter2D(gray, -1, sharpen_kernel)
    # Histogram eÅŸitleme
    equalized = cv2.equalizeHist(sharpened)
    return equalized

import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA device count: {torch.cuda.device_count()}")
print(f"Current CUDA device: {torch.cuda.current_device()}")

import supervision as sv
import numpy as np
from tqdm import tqdm
from sports.common.team import TeamClassifier

# Video KaynaÄŸÄ±
SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"

# Nesne Kimlikleri
BALL_ID = 0
GOALKEEPER_ID = 1
PLAYER_ID = 2
REFEREE_ID = 3

# Frame alma sÄ±klÄ±ÄŸÄ±
STRIDE = 30
frame_counter = 0  # Frame numarasÄ±nÄ± takip et

# Model ve Tracker TanÄ±mlama
tracker = sv.ByteTrack()
tracker.reset()
team_classifier = TeamClassifier(device="cuda")

# Modelin eÄŸitildiÄŸini kontrol etmek iÃ§in deÄŸiÅŸken
is_team_classifier_trained = False  # Modelin eÄŸitilip eÄŸitilmediÄŸini takip et

# Video karelerini almak iÃ§in generator
frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH, stride=STRIDE)

for frame in tqdm(frame_generator, desc="Processing video"):
    frame_counter += STRIDE  # Frame numarasÄ±nÄ± gÃ¼ncelle

    # Oyuncu AlgÄ±lama Modelini Ã‡alÄ±ÅŸtÄ±r
    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)

    # EÄŸer hiÃ§bir nesne tespit edilmediyse, ekrana frame numarasÄ± yazdÄ±r
    if len(detections) == 0:
        print(f"âš ï¸ Warning: No objects detected at frame {frame_counter}")

        # BoÅŸ bir Detections nesnesi oluÅŸtur
        all_detections = sv.Detections(
            xyxy=np.empty((0, 4)),
            class_id=np.empty((0,)),
            confidence=np.empty((0,)),
            tracker_id=np.empty((0,))
        )
    else:
        # Top tespitlerini ve diÄŸer objeleri ayÄ±r
        ball_detections = detections[detections.class_id == BALL_ID]
        all_detections = detections[detections.class_id != BALL_ID]

        # EÄŸer oyuncu, hakem veya kaleci yoksa sadece tracking devam etsin
        if len(all_detections) > 0:
            all_detections = tracker.update_with_detections(detections=all_detections)

        # Oyuncu, kaleci ve hakemleri ayÄ±r
        players_detections = all_detections[all_detections.class_id == PLAYER_ID]
        # Forma numarasÄ± tespiti yapÄ±lmamÄ±ÅŸ oyuncular iÃ§in OCR Ã§alÄ±ÅŸtÄ±r
        for i, tracker_id in enumerate(players_detections.tracker_id):
          if tracker_id not in tracker_id_to_jersey_number:
              crop = crop_upper_body(players_detections.xyxy[i], frame)

              # CROPâ€™U DEBUG Ä°Ã‡Ä°N KAYDET
              debug_path = f"debug_crops/frame{frame_counter}_id{tracker_id}.jpg"
              save_crop_with_label(crop, debug_path, f"F{frame_counter} ID:{tracker_id}")


              # OCR iÅŸlemi
              processed_crop = preprocess_for_ocr(crop)
              jersey_number = get_jersey_number_from_crop(processed_crop)
              if jersey_number is not None:
                  ocr_results_buffer[tracker_id].append(jersey_number)

              # OCR SonuÃ§larÄ±nÄ± deÄŸerlendirme
              if len(ocr_results_buffer[tracker_id]) >= 5:
                  most_common = Counter(ocr_results_buffer[tracker_id]).most_common(1)[0]
                  number, count = most_common
                  if count >= 2:
                      tracker_id_to_jersey_number[tracker_id] = number
                      print(f"âœ… Jersey #{number} assigned to Tracker ID {tracker_id}")
                  else:
                      print(f"ðŸ” OCR results for {tracker_id}: {ocr_results_buffer[tracker_id]}")
              else:
                  print(f"ðŸ” Buffering OCR for Tracker ID {tracker_id}: {ocr_results_buffer[tracker_id]}")


        goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]
        referees_detections = all_detections[all_detections.class_id == REFEREE_ID]

        # OyuncularÄ±n takÄ±mÄ±nÄ± belirle
        if len(players_detections) > 0:
            players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]

            # EÄŸer crop edilen oyuncu gÃ¶rÃ¼ntÃ¼leri varsa, modelin eÄŸitilip eÄŸitilmediÄŸini kontrol et
            if len(players_crops) > 1:
                if not is_team_classifier_trained:
                    print(f"ðŸ”„ Training team classifier with {len(players_crops)} samples...")
                    team_classifier.fit(players_crops)
                    is_team_classifier_trained = True  # Modelin eÄŸitildiÄŸini iÅŸaretle

                # Model eÄŸitildiyse tahmin yap
                if is_team_classifier_trained:
                    players_detections.class_id = team_classifier.predict(players_crops)
            else:
                print(f"âš ï¸ Warning: Skipping team classification at frame {frame_counter} (Not enough data)")

        # Kalecilerin takÄ±mÄ±nÄ± belirle
        if len(goalkeepers_detections) > 0:
            goalkeepers_detections.class_id = resolve_goalkeepers_team_id(players_detections, goalkeepers_detections)

        # Hakemleri ayrÄ± kategoriye al
        referees_detections.class_id -= 1

        # Tespitleri birleÅŸtir
        all_detections = sv.Detections.merge([players_detections, goalkeepers_detections, referees_detections])

    # Takip ID'lerini ekleyerek etiketleme (BoÅŸluk kontrolÃ¼ eklendi)
    labels = [
      f"#{tracker_id_to_jersey_number.get(tracker_id, '??')}"
      for tracker_id in all_detections.tracker_id
    ] if len(all_detections) > 0 else []


    # GÃ¶rselleÅŸtirme
    annotated_frame = frame.copy()

    if len(all_detections) > 0:
        ellipse_annotator = sv.EllipseAnnotator(
            color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
            thickness=2
        )
        label_annotator = sv.LabelAnnotator(
            color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
            text_color=sv.Color.from_hex('#000000'),
            text_position=sv.Position.BOTTOM_CENTER
        )
        triangle_annotator = sv.TriangleAnnotator(
            color=sv.Color.from_hex('#FFD700'),
            base=25,
            height=21,
            outline_thickness=1
        )

        annotated_frame = ellipse_annotator.annotate(scene=annotated_frame, detections=all_detections)
        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=all_detections, labels=labels)
        annotated_frame = triangle_annotator.annotate(scene=annotated_frame, detections=ball_detections)

    # Frame'yi gÃ¶ster
    sv.plot_image(annotated_frame)

print("âœ… Video Processing Completed Successfully!")

# Forma numarasÄ± eÅŸleÅŸmelerini kaydet
with open(JERSEY_MAPPING_PATH, "w") as f:
    json.dump(tracker_id_to_jersey_number, f)

import supervision as sv

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
BALL_ID = 0
GOALKEEPER_ID = 1
PLAYER_ID = 2
REFEREE_ID = 3

ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    text_color=sv.Color.from_hex('#000000'),
    text_position=sv.Position.BOTTOM_CENTER
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=25,
    height=21,
    outline_thickness=1
)

tracker = sv.ByteTrack()
tracker.reset()

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
detections = sv.Detections.from_inference(result)

ball_detections = detections[detections.class_id == BALL_ID]
ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

all_detections = detections[detections.class_id != BALL_ID]
all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
all_detections = tracker.update_with_detections(detections=all_detections)

goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]
players_detections = all_detections[all_detections.class_id == PLAYER_ID]
referees_detections = all_detections[all_detections.class_id == REFEREE_ID]

players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]
players_detections.class_id = team_classifier.predict(players_crops)

goalkeepers_detections.class_id = resolve_goalkeepers_team_id(
    players_detections, goalkeepers_detections)

referees_detections.class_id -= 1

all_detections = sv.Detections.merge([
    players_detections, goalkeepers_detections, referees_detections])

labels = [
    f"#{tracker_id}"
    for tracker_id
    in all_detections.tracker_id
]

all_detections.class_id = all_detections.class_id.astype(int)

annotated_frame = frame.copy()
annotated_frame = ellipse_annotator.annotate(
    scene=annotated_frame,
    detections=all_detections)
annotated_frame = label_annotator.annotate(
    scene=annotated_frame,
    detections=all_detections,
    labels=labels)
annotated_frame = triangle_annotator.annotate(
    scene=annotated_frame,
    detections=ball_detections)

sv.plot_image(annotated_frame)

"""## pitch keypoint detection"""

from inference import get_model
from google.colab import userdata

ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')
FIELD_DETECTION_MODEL_ID = "football-field-detection-f07vi/14"
FIELD_DETECTION_MODEL = get_model(model_id=FIELD_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)

import supervision as sv

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"

vertex_annotator = sv.VertexAnnotator(
    color=sv.Color.from_hex('#FF1493'),
    radius=8)

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
key_points = sv.KeyPoints.from_inference(result)

annotated_frame = frame.copy()
annotated_frame = vertex_annotator.annotate(
    scene=annotated_frame,
    key_points=key_points)

sv.plot_image(annotated_frame)

import supervision as sv
import random
import cv2

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"

# Video capture aÃ§
cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
print(f"Toplam frame sayÄ±sÄ±: {total_frames}")

# Rastgele bir frame indeksi seÃ§
random_frame_index = random.randint(0, total_frames - 1)
print(f"SeÃ§ilen rastgele frame indeksi: {random_frame_index}")

# SeÃ§ilen frame'e git
cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame_index)
ret, frame = cap.read()
cap.release()

if ret:
    # OpenCV BGR -> RGB dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Vertex annotator oluÅŸtur
    vertex_annotator = sv.VertexAnnotator(
        color=sv.Color.from_hex('#FF1493'),
        radius=8)

    # Modeli Ã§alÄ±ÅŸtÄ±r
    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    key_points = sv.KeyPoints.from_inference(result)

    # Frame'i annotate et
    annotated_frame = frame.copy()
    annotated_frame = vertex_annotator.annotate(
        scene=annotated_frame,
        key_points=key_points)

    # Sonucu gÃ¶ster
    sv.plot_image(annotated_frame)
else:
    print("Frame okunamadÄ±!")

"""**Note:** Notice that some of the keypoints we detected are in incorrect locations. These are keypoints with a low confidence level. Let's filter out these keypoints and keep only the ones the model is confident about.

## filter low confidence keypoints
"""

import supervision as sv

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"

vertex_annotator = sv.VertexAnnotator(
    color=sv.Color.from_hex('#FF1493'),
    radius=8)

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=200)
frame = next(frame_generator)

result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
key_points = sv.KeyPoints.from_inference(result)

filter = key_points.confidence[0] > 0.5
frame_reference_points = key_points.xy[0][filter]
frame_reference_key_points = sv.KeyPoints(
    xy=frame_reference_points[np.newaxis, ...])

annotated_frame = frame.copy()
annotated_frame = vertex_annotator.annotate(
    scene=annotated_frame,
    key_points=frame_reference_key_points)

sv.plot_image(annotated_frame)

import supervision as sv
import numpy as np
import random

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"

# Video bilgilerini al
video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
total_frames = video_info.total_frames
print(f"Toplam frame sayÄ±sÄ±: {total_frames}")

# Rastgele bir frame indeksi seÃ§
random_frame_index = random.randint(0, total_frames - 1)
print(f"SeÃ§ilen rastgele frame indeksi: {random_frame_index}")

vertex_annotator = sv.VertexAnnotator(
    color=sv.Color.from_hex('#FF1493'),
    radius=8)

# Rastgele seÃ§ilen frame'den baÅŸla
frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=random_frame_index)
frame = next(frame_generator)

# Modeli Ã§alÄ±ÅŸtÄ±r
result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
key_points = sv.KeyPoints.from_inference(result)

# GÃ¼ven deÄŸeri 0.5'ten bÃ¼yÃ¼k noktalarÄ± filtrele
filter = key_points.confidence[0] > 0.5
frame_reference_points = key_points.xy[0][filter]
frame_reference_key_points = sv.KeyPoints(
    xy=frame_reference_points[np.newaxis, ...])

# Frame'i annotate et
annotated_frame = frame.copy()
annotated_frame = vertex_annotator.annotate(
    scene=annotated_frame,
    key_points=frame_reference_key_points)

# Sonucu gÃ¶ster
sv.plot_image(annotated_frame)

"""## project pitch lines on frame

**Note:** The [sports](https://github.com/roboflow/sports) repository  [`SoccerPitchConfiguration`](https://github.com/roboflow/sports/blob/06053616f1f8a8ae1fa936eb00dcdc2e4f888bb1/sports/configs/soccer.py#L6)
"""

from sports.annotators.soccer import draw_pitch
from sports.configs.soccer import SoccerPitchConfiguration

CONFIG = SoccerPitchConfiguration()

annotated_frame = draw_pitch(CONFIG)

sv.plot_image(annotated_frame)

"""**Note:** It's time to utilize the keypoint pairs located on the camera perspective plane and the football pitch plane. The [sports](https://github.com/roboflow/sports) repository includes a [`ViewTransformer`](https://github.com/roboflow/sports/blob/06053616f1f8a8ae1fa936eb00dcdc2e4f888bb1/sports/common/view.py#L7), which employs homography for perspective transformation."""

import numpy as np
import supervision as sv
from sports.common.view import ViewTransformer

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"

edge_annotator = sv.EdgeAnnotator(
    color=sv.Color.from_hex('#00BFFF'),
    thickness=2, edges=CONFIG.edges)
vertex_annotator = sv.VertexAnnotator(
    color=sv.Color.from_hex('#FF1493'),
    radius=8)
vertex_annotator_2 = sv.VertexAnnotator(
    color=sv.Color.from_hex('#00BFFF'),
    radius=8)

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=200)
frame = next(frame_generator)

result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
key_points = sv.KeyPoints.from_inference(result)

filter = key_points.confidence[0] > 0.5
frame_reference_points = key_points.xy[0][filter]
frame_reference_key_points = sv.KeyPoints(
    xy=frame_reference_points[np.newaxis, ...])

pitch_reference_points = np.array(CONFIG.vertices)[filter]

transformer = ViewTransformer(
    source=pitch_reference_points,
    target=frame_reference_points
)

pitch_all_points = np.array(CONFIG.vertices)
frame_all_points = transformer.transform_points(points=pitch_all_points)

frame_all_key_points = sv.KeyPoints(xy=frame_all_points[np.newaxis, ...])

annotated_frame = frame.copy()
annotated_frame = edge_annotator.annotate(
    scene=annotated_frame,
    key_points=frame_all_key_points)
annotated_frame = vertex_annotator_2.annotate(
    scene=annotated_frame,
    key_points=frame_all_key_points)
annotated_frame = vertex_annotator.annotate(
    scene=annotated_frame,
    key_points=frame_reference_key_points)

sv.plot_image(annotated_frame)

"""## project ball, players and referies on pitch"""

import supervision as sv
from tqdm import tqdm
from sports.common.team import TeamClassifier

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
PLAYER_ID = 2
STRIDE = 30

frame_generator = sv.get_video_frames_generator(
    source_path=SOURCE_VIDEO_PATH, stride=STRIDE)

crops = []
for frame in tqdm(frame_generator, desc='collecting crops'):
    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)
    players_detections = detections[detections.class_id == PLAYER_ID]
    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]
    crops += players_crops

team_classifier = TeamClassifier(device="cuda")
team_classifier.fit(crops)

import cv2
from typing import Optional

def draw_pitch_voronoi_diagram_2(
    config: SoccerPitchConfiguration,
    team_1_xy: np.ndarray,
    team_2_xy: np.ndarray,
    team_1_color: sv.Color = sv.Color.RED,
    team_2_color: sv.Color = sv.Color.WHITE,
    opacity: float = 0.5,
    padding: int = 50,
    scale: float = 0.1,
    pitch: Optional[np.ndarray] = None
) -> np.ndarray:
    """
    Draws a Voronoi diagram on a soccer pitch representing the control areas of two
    teams with smooth color transitions.

    Args:
        config (SoccerPitchConfiguration): Configuration object containing the
            dimensions and layout of the pitch.
        team_1_xy (np.ndarray): Array of (x, y) coordinates representing the positions
            of players in team 1.
        team_2_xy (np.ndarray): Array of (x, y) coordinates representing the positions
            of players in team 2.
        team_1_color (sv.Color, optional): Color representing the control area of
            team 1. Defaults to sv.Color.RED.
        team_2_color (sv.Color, optional): Color representing the control area of
            team 2. Defaults to sv.Color.WHITE.
        opacity (float, optional): Opacity of the Voronoi diagram overlay.
            Defaults to 0.5.
        padding (int, optional): Padding around the pitch in pixels.
            Defaults to 50.
        scale (float, optional): Scaling factor for the pitch dimensions.
            Defaults to 0.1.
        pitch (Optional[np.ndarray], optional): Existing pitch image to draw the
            Voronoi diagram on. If None, a new pitch will be created. Defaults to None.

    Returns:
        np.ndarray: Image of the soccer pitch with the Voronoi diagram overlay.
    """
    if pitch is None:
        pitch = draw_pitch(
            config=config,
            padding=padding,
            scale=scale
        )

    scaled_width = int(config.width * scale)
    scaled_length = int(config.length * scale)

    voronoi = np.zeros_like(pitch, dtype=np.uint8)

    team_1_color_bgr = np.array(team_1_color.as_bgr(), dtype=np.uint8)
    team_2_color_bgr = np.array(team_2_color.as_bgr(), dtype=np.uint8)

    y_coordinates, x_coordinates = np.indices((
        scaled_width + 2 * padding,
        scaled_length + 2 * padding
    ))

    y_coordinates -= padding
    x_coordinates -= padding

    def calculate_distances(xy, x_coordinates, y_coordinates):
        return np.sqrt((xy[:, 0][:, None, None] * scale - x_coordinates) ** 2 +
                       (xy[:, 1][:, None, None] * scale - y_coordinates) ** 2)

    distances_team_1 = calculate_distances(team_1_xy, x_coordinates, y_coordinates)
    distances_team_2 = calculate_distances(team_2_xy, x_coordinates, y_coordinates)

    min_distances_team_1 = np.min(distances_team_1, axis=0)
    min_distances_team_2 = np.min(distances_team_2, axis=0)

    # Increase steepness of the blend effect
    steepness = 15  # Increased steepness for sharper transition
    distance_ratio = min_distances_team_2 / np.clip(min_distances_team_1 + min_distances_team_2, a_min=1e-5, a_max=None)
    blend_factor = np.tanh((distance_ratio - 0.5) * steepness) * 0.5 + 0.5

    # Create the smooth color transition
    for c in range(3):  # Iterate over the B, G, R channels
        voronoi[:, :, c] = (blend_factor * team_1_color_bgr[c] +
                            (1 - blend_factor) * team_2_color_bgr[c]).astype(np.uint8)

    overlay = cv2.addWeighted(voronoi, opacity, pitch, 1 - opacity, 0)

    return overlay

import supervision as sv
from sports.annotators.soccer import (
    draw_pitch,
    draw_points_on_pitch,
    draw_pitch_voronoi_diagram
)

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
BALL_ID = 0
GOALKEEPER_ID = 1
PLAYER_ID = 2
REFEREE_ID = 3

ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),
    text_color=sv.Color.from_hex('#000000'),
    text_position=sv.Position.BOTTOM_CENTER
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=20, height=17
)

tracker = sv.ByteTrack()
tracker.reset()

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

# ball, goalkeeper, player, referee detection

result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
detections = sv.Detections.from_inference(result)

ball_detections = detections[detections.class_id == BALL_ID]
ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

all_detections = detections[detections.class_id != BALL_ID]
all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
all_detections = tracker.update_with_detections(detections=all_detections)

goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]
players_detections = all_detections[all_detections.class_id == PLAYER_ID]
referees_detections = all_detections[all_detections.class_id == REFEREE_ID]

# team assignment

players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]
players_detections.class_id = team_classifier.predict(players_crops)

goalkeepers_detections.class_id = resolve_goalkeepers_team_id(
    players_detections, goalkeepers_detections)

referees_detections.class_id -= 1

all_detections = sv.Detections.merge([
    players_detections, goalkeepers_detections, referees_detections])

# frame visualization

labels = [
    f"#{tracker_id}"
    for tracker_id
    in all_detections.tracker_id
]

all_detections.class_id = all_detections.class_id.astype(int)

annotated_frame = frame.copy()
annotated_frame = ellipse_annotator.annotate(
    scene=annotated_frame,
    detections=all_detections)
annotated_frame = label_annotator.annotate(
    scene=annotated_frame,
    detections=all_detections,
    labels=labels)
annotated_frame = triangle_annotator.annotate(
    scene=annotated_frame,
    detections=ball_detections)

sv.plot_image(annotated_frame)

players_detections = sv.Detections.merge([
    players_detections, goalkeepers_detections
])

# detect pitch key points

result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
key_points = sv.KeyPoints.from_inference(result)

# project ball, players and referies on pitch

filter = key_points.confidence[0] > 0.5
frame_reference_points = key_points.xy[0][filter]
pitch_reference_points = np.array(CONFIG.vertices)[filter]

transformer = ViewTransformer(
    source=frame_reference_points,
    target=pitch_reference_points
)

frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)

players_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
pitch_players_xy = transformer.transform_points(points=players_xy)

referees_xy = referees_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
pitch_referees_xy = transformer.transform_points(points=referees_xy)

# visualize video game-style radar view

annotated_frame = draw_pitch(CONFIG)
annotated_frame = draw_points_on_pitch(
    config=CONFIG,
    xy=pitch_ball_xy,
    face_color=sv.Color.WHITE,
    edge_color=sv.Color.BLACK,
    radius=10,
    pitch=annotated_frame)
annotated_frame = draw_points_on_pitch(
    config=CONFIG,
    xy=pitch_players_xy[players_detections.class_id == 0],
    face_color=sv.Color.from_hex('00BFFF'),
    edge_color=sv.Color.BLACK,
    radius=16,
    pitch=annotated_frame)
annotated_frame = draw_points_on_pitch(
    config=CONFIG,
    xy=pitch_players_xy[players_detections.class_id == 1],
    face_color=sv.Color.from_hex('FF1493'),
    edge_color=sv.Color.BLACK,
    radius=16,
    pitch=annotated_frame)
annotated_frame = draw_points_on_pitch(
    config=CONFIG,
    xy=pitch_referees_xy,
    face_color=sv.Color.from_hex('FFD700'),
    edge_color=sv.Color.BLACK,
    radius=16,
    pitch=annotated_frame)

sv.plot_image(annotated_frame)

# visualize voronoi diagram

annotated_frame = draw_pitch(CONFIG)
annotated_frame = draw_pitch_voronoi_diagram(
    config=CONFIG,
    team_1_xy=pitch_players_xy[players_detections.class_id == 0],
    team_2_xy=pitch_players_xy[players_detections.class_id == 1],
    team_1_color=sv.Color.from_hex('00BFFF'),
    team_2_color=sv.Color.from_hex('FF1493'),
    pitch=annotated_frame)

sv.plot_image(annotated_frame)

# visualize voronoi diagram with blend

annotated_frame = draw_pitch(
    config=CONFIG,
    background_color=sv.Color.WHITE,
    line_color=sv.Color.BLACK
)
annotated_frame = draw_pitch_voronoi_diagram_2(
    config=CONFIG,
    team_1_xy=pitch_players_xy[players_detections.class_id == 0],
    team_2_xy=pitch_players_xy[players_detections.class_id == 1],
    team_1_color=sv.Color.from_hex('00BFFF'),
    team_2_color=sv.Color.from_hex('FF1493'),
    pitch=annotated_frame)
annotated_frame = draw_points_on_pitch(
    config=CONFIG,
    xy=pitch_ball_xy,
    face_color=sv.Color.WHITE,
    edge_color=sv.Color.WHITE,
    radius=8,
    thickness=1,
    pitch=annotated_frame)
annotated_frame = draw_points_on_pitch(
    config=CONFIG,
    xy=pitch_players_xy[players_detections.class_id == 0],
    face_color=sv.Color.from_hex('00BFFF'),
    edge_color=sv.Color.WHITE,
    radius=16,
    thickness=1,
    pitch=annotated_frame)
annotated_frame = draw_points_on_pitch(
    config=CONFIG,
    xy=pitch_players_xy[players_detections.class_id == 1],
    face_color=sv.Color.from_hex('FF1493'),
    edge_color=sv.Color.WHITE,
    radius=16,
    thickness=1,
    pitch=annotated_frame)

sv.plot_image(annotated_frame)

"""## ball tracking"""

from collections import deque
import supervision as sv
from sports.annotators.soccer import draw_pitch, draw_points_on_pitch

SOURCE_VIDEO_PATH = "/content/121364_0.mp4"
BALL_ID = 0
MAXLEN = 5

video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)

path_raw = []
M = deque(maxlen=MAXLEN)

for frame in tqdm(frame_generator, total=video_info.total_frames):

    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)

    ball_detections = detections[detections.class_id == BALL_ID]
    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    key_points = sv.KeyPoints.from_inference(result)

    filter = key_points.confidence[0] > 0.5
    frame_reference_points = key_points.xy[0][filter]
    pitch_reference_points = np.array(CONFIG.vertices)[filter]

    transformer = ViewTransformer(
        source=frame_reference_points,
        target=pitch_reference_points
    )
    M.append(transformer.m)
    transformer.m = np.mean(np.array(M), axis=0)

    frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
    pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)

    path_raw.append(pitch_ball_xy)

path = [
    np.empty((0, 2), dtype=np.float32) if coorinates.shape[0] >= 2 else coorinates
    for coorinates
    in path_raw
]

path = [coorinates.flatten() for coorinates in path]

from sports.annotators.soccer import draw_paths_on_pitch

annotated_frame = draw_pitch(CONFIG)
annotated_frame = draw_paths_on_pitch(
    config=CONFIG,
    paths=[path],
    color=sv.Color.WHITE,
    pitch=annotated_frame)

sv.plot_image(annotated_frame)

from typing import List, Union

def replace_outliers_based_on_distance(
    positions: List[np.ndarray],
    distance_threshold: float
) -> List[np.ndarray]:
    last_valid_position: Union[np.ndarray, None] = None
    cleaned_positions: List[np.ndarray] = []

    for position in positions:
        if len(position) == 0:
            # If the current position is already empty, just add it to the cleaned positions
            cleaned_positions.append(position)
        else:
            if last_valid_position is None:
                # If there's no valid last position, accept the first valid one
                cleaned_positions.append(position)
                last_valid_position = position
            else:
                # Calculate the distance from the last valid position
                distance = np.linalg.norm(position - last_valid_position)
                if distance > distance_threshold:
                    # Replace with empty array if the distance exceeds the threshold
                    cleaned_positions.append(np.array([], dtype=np.float64))
                else:
                    cleaned_positions.append(position)
                    last_valid_position = position

    return cleaned_positions

MAX_DISTANCE_THRESHOLD = 500

path = replace_outliers_based_on_distance(path, MAX_DISTANCE_THRESHOLD)

from sports.annotators.soccer import draw_paths_on_pitch

annotated_frame = draw_pitch(CONFIG)
annotated_frame = draw_paths_on_pitch(
    config=CONFIG,
    paths=[path],
    color=sv.Color.WHITE,
    pitch=annotated_frame)

sv.plot_image(annotated_frame)

"""### objelerin nokta halinde akÄ±ÅŸ videosu"""

# Commented out IPython magic to ensure Python compatibility.
import supervision as sv
import numpy as np
import cv2
from sports.common.view import ViewTransformer
from sports.annotators.soccer import draw_pitch, draw_points_on_pitch
from sports.configs.soccer import SoccerPitchConfiguration
from tqdm import tqdm
import os

# Model ID'leri
ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')
PLAYER_DETECTION_MODEL_ID = "football-players-detection-3zvbc/12"
FIELD_DETECTION_MODEL_ID = "football-field-detection-f07vi/14"

# Modelleri yÃ¼kle
from inference import get_model
from google.colab import userdata


from sports.common.team import TeamClassifier
import matplotlib.pyplot as plt
import numpy as np
import supervision as sv
from tqdm import tqdm

# Ana kod
import cv2

# Ã–nce matplotlib'i inline moda geÃ§irin (Colab iÃ§in)
# %matplotlib inline

import supervision as sv


import os

"""###### AdÄ±m 1: Videoyu Kaydetmek iÃ§in HazÄ±rlÄ±k"""

# Dosya yollarÄ±
SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_11_fraport-tav-antalyaspor_besiktas.mp4"
OUTPUT_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/betul/analysis_output.mp4"

# Ã‡Ä±ktÄ± dizinini kontrol et ve oluÅŸtur
output_dir = os.path.dirname(OUTPUT_VIDEO_PATH)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"Ã‡Ä±ktÄ± dizini oluÅŸturuldu: {output_dir}")

# Nesne ID'leri
BALL_ID = 0
GOALKEEPER_ID = 1
PLAYER_ID = 2
REFEREE_ID = 3

# Ã‡Ä±ktÄ± video Ã¶zellikleri
STRIDE = 5  # Her 5 frame'de bir iÅŸlem yapacak (iÅŸlem hÄ±zÄ± iÃ§in)
CONFIG = SoccerPitchConfiguration()

# Video bilgilerini al
video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
fps = video_info.fps / STRIDE  # Ã‡Ä±ktÄ± FPS'i
height, width = video_info.height, video_info.width

print("Temel kurulum tamamlandÄ±!")
print(f"Video boyutu: {width}x{height}, FPS: {video_info.fps}")
print(f"Toplam frame sayÄ±sÄ±: {video_info.total_frames}")
print(f"Ã‡Ä±ktÄ± video boyutu: {width}x{height}, FPS: {fps}")

"""###### AdÄ±m 2: Model yÃ¼klemelerini kontrol edelim"""

# Model ID'leri
ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')
PLAYER_DETECTION_MODEL_ID = "football-players-detection-3zvbc/12"
FIELD_DETECTION_MODEL_ID = "football-field-detection-f07vi/14"

# Modelleri yÃ¼kle
from inference import get_model
from google.colab import userdata

print("Modeller yÃ¼kleniyor...")

try:
    PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)
    print("âœ… Oyuncu tespit modeli baÅŸarÄ±yla yÃ¼klendi")
except Exception as e:
    print(f"âŒ Oyuncu tespit modeli yÃ¼klenirken hata: {e}")

try:
    FIELD_DETECTION_MODEL = get_model(model_id=FIELD_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)
    print("âœ… Saha tespit modeli baÅŸarÄ±yla yÃ¼klendi")
except Exception as e:
    print(f"âŒ Saha tespit modeli yÃ¼klenirken hata: {e}")

# Test et - bir frame al ve modeli Ã§alÄ±ÅŸtÄ±r
print("\nModel testleri yapÄ±lÄ±yor...")

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
test_frame = next(frame_generator)

try:
    player_result = PLAYER_DETECTION_MODEL.infer(test_frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(player_result)
    print(f"âœ… Oyuncu tespit modeli test edildi. {len(detections)} nesne tespit edildi.")
except Exception as e:
    print(f"âŒ Oyuncu tespit modeli test edilirken hata: {e}")

try:
    field_result = FIELD_DETECTION_MODEL.infer(test_frame, confidence=0.3)[0]
    key_points = sv.KeyPoints.from_inference(field_result)
    print(f"âœ… Saha tespit modeli test edildi. {len(key_points.xy[0])} nokta tespit edildi.")
except Exception as e:
    print(f"âŒ Saha tespit modeli test edilirken hata: {e}")

"""###### AdÄ±m 3: Team Classifier eÄŸitimini kontrol edelim"""

# Ã–rnek bir frame Ã¼zerinde gÃ¶rselleÅŸtirme yapalÄ±m
def visualize_detections_with_teams(frame, detections, team_predictions):
    """
    Frame Ã¼zerinde tespit edilen oyuncularÄ± takÄ±mlarÄ±na gÃ¶re farklÄ± renklerle iÅŸaretler
    """
    # TakÄ±m renklerini belirle (0: takÄ±m1-kÄ±rmÄ±zÄ±, 1: takÄ±m2-mavi)
    colors = {
        0: (255, 0, 0),  # KÄ±rmÄ±zÄ± - TakÄ±m 1
        1: (0, 0, 255),  # Mavi - TakÄ±m 2
        -1: (128, 128, 128)  # Gri - SÄ±nÄ±flandÄ±rÄ±lamayan
    }

    # GÃ¶rselleÅŸtirme iÃ§in annotator oluÅŸtur
    annotated_frame = frame.copy()

    # Her bir tespit iÃ§in
    for i, xyxy in enumerate(detections.xyxy):
        # EÄŸer oyuncu tespiti varsa
        if i < len(team_predictions):
            team_id = team_predictions[i]
            color = colors[team_id]

            # DikdÃ¶rtgen Ã§iz
            x1, y1, x2, y2 = map(int, xyxy)
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

            # TakÄ±m bilgisini yaz
            team_text = f"Takim {team_id+1}"
            cv2.putText(annotated_frame, team_text, (x1, y1-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    return annotated_frame



print("Team Classifier test ediliyor...")
try:
    # Ã–rnek bir sÄ±nÄ±flandÄ±rÄ±cÄ± oluÅŸtur
    team_classifier = TeamClassifier(device="cpu")
    print("âœ… Team Classifier sÄ±nÄ±fÄ± baÅŸarÄ±yla oluÅŸturuldu")
except Exception as e:
    print(f"âŒ Team Classifier oluÅŸturulurken hata: {e}")

print("\nTakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± iÃ§in veri toplanÄ±yor...")
crops = []
all_detections = []  # TÃ¼m tespitleri saklayalÄ±m
all_frames = []      # Tespitlerin yapÄ±ldÄ±ÄŸÄ± frameleri saklayalÄ±m

frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, stride=30)

try:
    for i, frame in enumerate(tqdm(frame_generator)):
        if i > 100:  # Test iÃ§in sadece 20 frame'den Ã¶rnek topla
            break
        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        detections = sv.Detections.from_inference(result)
        players = detections[detections.class_id == PLAYER_ID]

        # Oyuncu tespitlerini ve frameleri sakla
        if len(players) > 0:
            player_crops = [sv.crop_image(frame, xyxy) for xyxy in players.xyxy]
            crops += player_crops
            all_detections.append(players)
            all_frames.append(frame.copy())

    print(f"âœ… {len(crops)} oyuncu gÃ¶rÃ¼ntÃ¼sÃ¼ toplandÄ±")

    # Team Classifier'Ä± eÄŸit
    print("\nTakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± eÄŸitiliyor...")
    team_classifier.fit(crops)
    print("âœ… TakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± baÅŸarÄ±yla eÄŸitildi!")

    # Her bir frame iÃ§in takÄ±m sÄ±nÄ±flandÄ±rmasÄ± yap ve gÃ¶rselleÅŸtir
    print("\nTespit edilen oyuncular takÄ±mlara gÃ¶re iÅŸaretleniyor...")

    for frame_idx, (frame, detections) in enumerate(zip(all_frames, all_detections)):
        # Bu framedeki oyuncu kÄ±rpmalarÄ±
        frame_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]

        if len(frame_crops) > 0:
            # TakÄ±m tahminlerini al
            team_predictions = team_classifier.predict(frame_crops)

            # GÃ¶rselleÅŸtir
            annotated_frame = visualize_detections_with_teams(frame, detections, team_predictions)

            # Sonucu gÃ¶ster
            plt.figure(figsize=(12, 8))
            plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))
            plt.title(f"Frame {frame_idx+1} - TakÄ±m SÄ±nÄ±flandÄ±rmasÄ±")
            plt.axis('off')
            plt.show()

            # Sadece bir Ã¶rnek gÃ¶sterelim (isteÄŸe baÄŸlÄ± olarak kaldÄ±rÄ±labilir)
            if frame_idx == 0:
                print(f"Frame {frame_idx+1} takÄ±m tahminleri: {team_predictions}")

    print("âœ… TakÄ±m tespitleri tamamlandÄ±!")

except Exception as e:
    print(f"âŒ Ä°ÅŸlem sÄ±rasÄ±nda hata: {e}")

"""###### AdÄ±m 4: YardÄ±mcÄ± fonksiyonlarÄ± tanÄ±mlayalÄ±m ve test edelim (Team Classifier dahil)"""

# Renk paletini tanÄ±mla
COLORS = {
    "team_0": (255, 0, 0),       # KÄ±rmÄ±zÄ± - TakÄ±m 1
    "team_1": (0, 0, 255),       # Mavi - TakÄ±m 2
    "goalkeeper_0": (200, 50, 50),  # Koyu kÄ±rmÄ±zÄ± - TakÄ±m 1 kaleci
    "goalkeeper_1": (50, 50, 200),  # Koyu mavi - TakÄ±m 2 kaleci
    "referee": (255, 255, 0),    # SarÄ± - Hakem
    "ball": (0, 255, 0)          # YeÅŸil - Top
}

def visualize_detections_with_teams(frame, players, goalkeepers, referees, ball_detections, team_ids, gk_team_ids):
    """
    Frame Ã¼zerinde tespit edilen oyuncularÄ±, kalecileri, hakemleri ve topu gÃ¶sterir
    """
    annotated_frame = frame.copy()

    # Tespit sayÄ±larÄ±nÄ± yazdÄ±r
    print(f"GÃ¶rselleÅŸtiriliyor: {len(players)} oyuncu, {len(goalkeepers)} kaleci, {len(referees)} hakem, {len(ball_detections)} top")

    # OyuncularÄ± Ã§iz
    for i, xyxy in enumerate(players.xyxy):
        if i < len(team_ids):
            team_id = team_ids[i]
            color = COLORS[f"team_{team_id}"]

            # DikdÃ¶rtgen Ã§iz
            x1, y1, x2, y2 = map(int, xyxy)
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

            # TakÄ±m ve ID bilgisini yaz
            track_id = players.tracker_id[i] if hasattr(players, 'tracker_id') else i
            team_text = f"T{team_id+1}-ID:{track_id}"
            cv2.putText(annotated_frame, team_text, (x1, y1-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Kalecileri Ã§iz
    for i, xyxy in enumerate(goalkeepers.xyxy):
        if i < len(gk_team_ids):
            team_id = gk_team_ids[i]
            color = COLORS[f"goalkeeper_{team_id}"]

            # DikdÃ¶rtgen Ã§iz
            x1, y1, x2, y2 = map(int, xyxy)
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

            # Kaleci ve ID bilgisini yaz
            track_id = goalkeepers.tracker_id[i] if hasattr(goalkeepers, 'tracker_id') else i
            gk_text = f"GK-T{team_id+1}-ID:{track_id}"
            cv2.putText(annotated_frame, gk_text, (x1, y1-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Hakemleri Ã§iz
    for i, xyxy in enumerate(referees.xyxy):
        color = COLORS["referee"]

        # DikdÃ¶rtgen Ã§iz
        x1, y1, x2, y2 = map(int, xyxy)
        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

        # Hakem ve ID bilgisini yaz
        track_id = referees.tracker_id[i] if hasattr(referees, 'tracker_id') else i
        ref_text = f"REF-ID:{track_id}"
        cv2.putText(annotated_frame, ref_text, (x1, y1-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Topu Ã§iz
    for i, xyxy in enumerate(ball_detections.xyxy):
        color = COLORS["ball"]

        # DikdÃ¶rtgen Ã§iz
        x1, y1, x2, y2 = map(int, xyxy)
        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

        # Top bilgisini yaz
        ball_text = "BALL"
        cv2.putText(annotated_frame, ball_text, (x1, y1-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    return annotated_frame

def resolve_goalkeepers_team_id(players_detections, goalkeepers_detections, team_ids):
    """Kalecilerin takÄ±mlarÄ±nÄ±, en yakÄ±n oyuncularÄ±n takÄ±mlarÄ±nÄ± referans alarak belirler."""
    if len(goalkeepers_detections) == 0 or len(players_detections) == 0:
        return np.array([])

    gk_team_ids = []

    for gk_idx in range(len(goalkeepers_detections)):
        gk_xy = goalkeepers_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)[gk_idx]

        player_distances = []
        for player_idx in range(len(players_detections)):
            player_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)[player_idx]
            distance = np.sqrt(np.sum((gk_xy - player_xy) ** 2))
            team_id = team_ids[player_idx]
            player_distances.append((distance, team_id))

        # En yakÄ±n 3 oyuncuyu al
        player_distances.sort(key=lambda x: x[0])
        closest_teams = [team_id for _, team_id in player_distances[:3]]

        # En Ã§ok hangi takÄ±m yakÄ±nsa o takÄ±mda
        if len(closest_teams) > 0:
            most_common_team = max(set(closest_teams), key=closest_teams.count)
            gk_team_ids.append(most_common_team)
        else:
            gk_team_ids.append(0)  # VarsayÄ±lan olarak 0. takÄ±m

    return np.array(gk_team_ids)

# Team Classifier test et
print("Team Classifier test ediliyor...")
try:
    team_classifier = TeamClassifier(device="cpu")  # EÄŸer GPU varsa "cuda" kullanabilirsiniz
    print("âœ… Team Classifier sÄ±nÄ±fÄ± baÅŸarÄ±yla oluÅŸturuldu")
except Exception as e:
    print(f"âŒ Team Classifier oluÅŸturulurken hata: {e}")

# Tracker hazÄ±rla
tracker = sv.ByteTrack()
tracker.reset()

# Veri toplama ve eÄŸitim
print("\nTakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± iÃ§in veri toplanÄ±yor...")
crops = []
frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, stride=30)  # HÄ±zlÄ± tarama iÃ§in 30 frame atlayarak

try:
    for i, frame in enumerate(tqdm(frame_generator)):
        if i > 20:  # Test iÃ§in sadece 20 frameden Ã¶rnek topla
            break
        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        detections = sv.Detections.from_inference(result)
        players = detections[detections.class_id == PLAYER_ID]
        player_crops = [sv.crop_image(frame, xyxy) for xyxy in players.xyxy]
        crops += player_crops

    print(f"âœ… {len(crops)} oyuncu gÃ¶rÃ¼ntÃ¼sÃ¼ toplandÄ±")

    # Ã–rnek oyuncu gÃ¶rÃ¼ntÃ¼lerini gÃ¶relim
    if len(crops) > 0:
        display_count = min(5, len(crops))
        plt.figure(figsize=(15, 3))
        for i in range(display_count):
            plt.subplot(1, display_count, i+1)
            plt.imshow(cv2.cvtColor(crops[i], cv2.COLOR_BGR2RGB))
            plt.title(f"Oyuncu {i+1}")
            plt.axis('off')
        plt.show()
    else:
        print("âš ï¸ HiÃ§ oyuncu gÃ¶rÃ¼ntÃ¼sÃ¼ toplanamadÄ±!")

    # Team Classifier'Ä± eÄŸit
    print("\nTakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± eÄŸitiliyor...")
    team_classifier.fit(crops)
    print("âœ… TakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± baÅŸarÄ±yla eÄŸitildi!")

    # Åžimdi birkaÃ§ frame iÃ§in tespit ve gÃ¶rselleÅŸtirme yapalÄ±m
    print("\nFramelerde tespit ve gÃ¶rselleÅŸtirme yapÄ±lÄ±yor...")

    # Video'yu baÅŸtan aÃ§alÄ±m
    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, stride=5)  # 5 frame atlayarak

    for frame_idx, frame in enumerate(tqdm(frame_generator)):
        if frame_idx > 1000:  # 5 frame iÅŸle
            break

        # Tespitleri yap
        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        detections = sv.Detections.from_inference(result)

        # Top tespitlerini ayÄ±r
        ball_detections = detections[detections.class_id == BALL_ID]
        all_detections = detections[detections.class_id != BALL_ID]

        # Takip iÅŸlemi
        all_detections = tracker.update_with_detections(detections=all_detections)

        # Oyuncu, kaleci ve hakemleri ayÄ±r
        goalkeepers = all_detections[all_detections.class_id == GOALKEEPER_ID]
        players = all_detections[all_detections.class_id == PLAYER_ID]
        referees = all_detections[all_detections.class_id == REFEREE_ID]

        print(f"\nFrame {frame_idx+1} tespitleri:")
        print(f"  Toplar: {len(ball_detections)}, Kaleciler: {len(goalkeepers)}, Oyuncular: {len(players)}, Hakemler: {len(referees)}")

        # TakÄ±m sÄ±nÄ±flandÄ±rmalarÄ±
        if len(players) > 0:
            players_crops = [sv.crop_image(frame, xyxy) for xyxy in players.xyxy]
            team_ids = team_classifier.predict(players_crops)

            # Kaleci takÄ±m atamasÄ±
            gk_team_ids = []
            if len(goalkeepers) > 0:
                gk_team_ids = resolve_goalkeepers_team_id(players, goalkeepers, team_ids)

            # GÃ¶rselleÅŸtir
            annotated_frame = visualize_detections_with_teams(
                frame, players, goalkeepers, referees, ball_detections, team_ids, gk_team_ids
            )

            # Sonucu gÃ¶ster
            plt.figure(figsize=(12, 8))
            plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))
            plt.title(f"Frame {frame_idx+1} - TakÄ±m SÄ±nÄ±flandÄ±rmasÄ±")
            plt.axis('off')
            plt.savefig(f"/tmp/frame_{frame_idx+1}.png")  # Dosyaya kaydet
            plt.show()  # DoÄŸrudan gÃ¶ster

            # Ä°statistikleri yazdÄ±r
            if len(team_ids) > 0:
                team_count = np.bincount(team_ids, minlength=2)
                print(f"  TakÄ±m sayÄ±larÄ±: TakÄ±m 1: {team_count[0]}, TakÄ±m 2: {team_count[1]}")

            if len(gk_team_ids) > 0:
                print(f"  Kaleci takÄ±m atamasÄ±: {gk_team_ids}")
        else:
            print("  Bu framede oyuncu tespit edilemedi, gÃ¶rselleÅŸtirme yapÄ±lamÄ±yor.")

    print("âœ… Ä°ÅŸlemler tamamlandÄ±!")

except Exception as e:
    import traceback
    print(f"âŒ Ä°ÅŸlem sÄ±rasÄ±nda hata: {e}")
    print(traceback.format_exc())  # Tam hata izini yazdÄ±r

# Renk paletini tanÄ±mla
COLORS = {
    "team_0": (255, 0, 0),       # KÄ±rmÄ±zÄ± - TakÄ±m 1
    "team_1": (0, 0, 255),       # Mavi - TakÄ±m 2
    "goalkeeper_0": (200, 50, 50),  # Koyu kÄ±rmÄ±zÄ± - TakÄ±m 1 kaleci
    "goalkeeper_1": (50, 50, 200),  # Koyu mavi - TakÄ±m 2 kaleci
    "referee": (255, 255, 0),    # SarÄ± - Hakem
    "ball": (0, 255, 0)          # YeÅŸil - Top
}

def visualize_detections_with_teams(frame, players, goalkeepers, referees, ball_detections, team_ids=None, gk_team_ids=None):
    """
    Frame Ã¼zerinde tespit edilen oyuncularÄ±, kalecileri, hakemleri ve topu gÃ¶sterir.
    Oyuncu tespit edilmese bile orijinal frame'i dÃ¶ndÃ¼rÃ¼r.
    """
    annotated_frame = frame.copy()

    # OyuncularÄ± Ã§iz (eÄŸer tespit edilmiÅŸse)
    if len(players) > 0 and team_ids is not None:
        for i, xyxy in enumerate(players.xyxy):
            if i < len(team_ids):
                team_id = team_ids[i]
                color = COLORS[f"team_{team_id}"]

                # DikdÃ¶rtgen Ã§iz
                x1, y1, x2, y2 = map(int, xyxy)
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

                # TakÄ±m ve ID bilgisini yaz
                track_id = players.tracker_id[i] if hasattr(players, 'tracker_id') else i
                team_text = f"T{team_id+1}-ID:{track_id}"
                cv2.putText(annotated_frame, team_text, (x1, y1-10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Kalecileri Ã§iz (eÄŸer tespit edilmiÅŸse)
    if len(goalkeepers) > 0 and gk_team_ids is not None:
        for i, xyxy in enumerate(goalkeepers.xyxy):
            if i < len(gk_team_ids):
                team_id = gk_team_ids[i]
                color = COLORS[f"goalkeeper_{team_id}"]

                # DikdÃ¶rtgen Ã§iz
                x1, y1, x2, y2 = map(int, xyxy)
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

                # Kaleci ve ID bilgisini yaz
                track_id = goalkeepers.tracker_id[i] if hasattr(goalkeepers, 'tracker_id') else i
                gk_text = f"GK-T{team_id+1}-ID:{track_id}"
                cv2.putText(annotated_frame, gk_text, (x1, y1-10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Hakemleri Ã§iz
    for i, xyxy in enumerate(referees.xyxy):
        color = COLORS["referee"]

        # DikdÃ¶rtgen Ã§iz
        x1, y1, x2, y2 = map(int, xyxy)
        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

        # Hakem ve ID bilgisini yaz
        track_id = referees.tracker_id[i] if hasattr(referees, 'tracker_id') else i
        ref_text = f"REF-ID:{track_id}"
        cv2.putText(annotated_frame, ref_text, (x1, y1-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Topu Ã§iz
    for i, xyxy in enumerate(ball_detections.xyxy):
        color = COLORS["ball"]

        # DikdÃ¶rtgen Ã§iz
        x1, y1, x2, y2 = map(int, xyxy)
        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

        # Top bilgisini yaz
        ball_text = "BALL"
        cv2.putText(annotated_frame, ball_text, (x1, y1-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Tespit bilgisini Ã¼st kÃ¶ÅŸeye ekle
    detection_info = f"Toplar: {len(ball_detections)}, Kaleciler: {len(goalkeepers)}, Oyuncular: {len(players)}, Hakemler: {len(referees)}"
    cv2.putText(annotated_frame, detection_info, (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

    return annotated_frame

def resolve_goalkeepers_team_id(players_detections, goalkeepers_detections, team_ids):
    """Kalecilerin takÄ±mlarÄ±nÄ±, en yakÄ±n oyuncularÄ±n takÄ±mlarÄ±nÄ± referans alarak belirler."""
    if len(goalkeepers_detections) == 0 or len(players_detections) == 0:
        return np.array([])

    gk_team_ids = []

    for gk_idx in range(len(goalkeepers_detections)):
        gk_xy = goalkeepers_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)[gk_idx]

        player_distances = []
        for player_idx in range(len(players_detections)):
            player_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)[player_idx]
            distance = np.sqrt(np.sum((gk_xy - player_xy) ** 2))
            team_id = team_ids[player_idx]
            player_distances.append((distance, team_id))

        # En yakÄ±n 3 oyuncuyu al
        player_distances.sort(key=lambda x: x[0])
        closest_teams = [team_id for _, team_id in player_distances[:3]]

        # En Ã§ok hangi takÄ±m yakÄ±nsa o takÄ±mda
        if len(closest_teams) > 0:
            most_common_team = max(set(closest_teams), key=closest_teams.count)
            gk_team_ids.append(most_common_team)
        else:
            gk_team_ids.append(0)  # VarsayÄ±lan olarak 0. takÄ±m

    return np.array(gk_team_ids)

# Ana iÅŸlem fonksiyonu
def process_football_video(source_video_path, output_video_path, start_frame=2000, max_frames=1000):
    # ID tanÄ±mlarÄ±
    BALL_ID = 0
    GOALKEEPER_ID = 1
    PLAYER_ID = 2
    REFEREE_ID = 3

    # Team Classifier'Ä± oluÅŸtur
    print("Team Classifier oluÅŸturuluyor...")
    try:
        team_classifier = TeamClassifier(device="cpu")  # GPU varsa "cuda" kullanabilirsiniz
        print("âœ… Team Classifier sÄ±nÄ±fÄ± baÅŸarÄ±yla oluÅŸturuldu")
    except Exception as e:
        print(f"âŒ Team Classifier oluÅŸturulurken hata: {e}")
        return

    # Tracker hazÄ±rla
    tracker = sv.ByteTrack()
    tracker.reset()

    # Ã–nce eÄŸitim iÃ§in veri topla
    print("\nTakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± iÃ§in veri toplanÄ±yor...")
    crops = []

    # EÄŸitim iÃ§in video frameleri jeneratÃ¶rÃ¼ - baÅŸlangÄ±Ã§ noktamÄ±zdan Ã¶nceki kÄ±sÄ±mdan Ã¶rnekler
    # 10 frame atlayarak start_frame kadar frame iÃ§inden Ã¶rnekler al
    sample_frames = min(start_frame, 500)  # En fazla 500 frame veya start_frame kadarÄ±nÄ± Ã¶rnekle
    frame_generator = sv.get_video_frames_generator(source_video_path, stride=max(1, sample_frames // 50))

    # EÄŸitim iÃ§in maksimum 50 frame Ã¶rneÄŸi al
    for i, frame in enumerate(tqdm(frame_generator)):
        if i >= 50:  # En fazla 50 Ã¶rnek
            break
        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        detections = sv.Detections.from_inference(result)
        players = detections[detections.class_id == PLAYER_ID]
        player_crops = [sv.crop_image(frame, xyxy) for xyxy in players.xyxy]
        crops += player_crops

    print(f"âœ… {len(crops)} oyuncu gÃ¶rÃ¼ntÃ¼sÃ¼ toplandÄ±")

    # Team Classifier'Ä± eÄŸit
    if len(crops) > 0:
        print("\nTakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± eÄŸitiliyor...")
        team_classifier.fit(crops)
        print("âœ… TakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± baÅŸarÄ±yla eÄŸitildi!")
    else:
        print("âš ï¸ HiÃ§ oyuncu tespit edilemedi! TakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± eÄŸitilemedi.")
        return

    # Video bilgilerini al
    print("\nVideo bilgileri alÄ±nÄ±yor...")
    cap = cv2.VideoCapture(source_video_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # BaÅŸlangÄ±Ã§ frame'ini kontrol et
    if start_frame >= total_frames:
        print(f"âš ï¸ BaÅŸlangÄ±Ã§ frame'i ({start_frame}) toplam frame sayÄ±sÄ±ndan ({total_frames}) bÃ¼yÃ¼k!")
        cap.release()
        return

    # Ä°ÅŸlenecek frame sayÄ±sÄ±nÄ± kontrol et
    end_frame = min(start_frame + max_frames, total_frames)
    frames_to_process = end_frame - start_frame

    print(f"Video boyutu: {width}x{height}, FPS: {fps}, Toplam frame: {total_frames}")
    print(f"Ä°ÅŸlenecek frameler: {start_frame} - {end_frame-1} (toplam {frames_to_process} frame)")

    # Ã‡Ä±ktÄ± videosunu hazÄ±rla
    output_dir = os.path.dirname(output_video_path)
    if not os.path.exists(output_dir) and output_dir:
        os.makedirs(output_dir)

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    # Ä°stenen baÅŸlangÄ±Ã§ frame'ine git (OpenCV kullanarak)
    print(f"\n{start_frame}. frame'e atlanÄ±yor...")
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

    # Bu noktadan itibaren frameleri iÅŸle
    print("\nBelirtilen frameler iÅŸleniyor ve Ã§Ä±ktÄ± videosuna yazÄ±lÄ±yor...")
    frame_count = 0

    with tqdm(total=frames_to_process) as pbar:
        while cap.isOpened() and frame_count < frames_to_process:
            ret, frame = cap.read()
            if not ret:
                print(f"âš ï¸ {start_frame + frame_count}. frame okunamadÄ±. Video bitti veya hatalÄ±.")
                break

            # OpenCV BGR formatÄ±nÄ± RGB'ye Ã§evir
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Tespitleri yap
            result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
            detections = sv.Detections.from_inference(result)

            # Top tespitlerini ayÄ±r
            ball_detections = detections[detections.class_id == BALL_ID]
            all_detections = detections[detections.class_id != BALL_ID]

            # Takip iÅŸlemi
            all_detections = tracker.update_with_detections(detections=all_detections)

            # Oyuncu, kaleci ve hakemleri ayÄ±r
            goalkeepers = all_detections[all_detections.class_id == GOALKEEPER_ID]
            players = all_detections[all_detections.class_id == PLAYER_ID]
            referees = all_detections[all_detections.class_id == REFEREE_ID]

            # TakÄ±m sÄ±nÄ±flandÄ±rmalarÄ±
            team_ids = None
            gk_team_ids = None

            if len(players) > 0:
                players_crops = [sv.crop_image(frame, xyxy) for xyxy in players.xyxy]
                team_ids = team_classifier.predict(players_crops)

                # Kaleci takÄ±m atamasÄ±
                if len(goalkeepers) > 0:
                    gk_team_ids = resolve_goalkeepers_team_id(players, goalkeepers, team_ids)

            # GÃ¶rselleÅŸtir
            annotated_frame = visualize_detections_with_teams(
                frame, players, goalkeepers, referees, ball_detections, team_ids, gk_team_ids
            )

            # Frame'i BGR'ye Ã§evir (OpenCV iÃ§in)
            annotated_frame_bgr = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)

            # Ã‡Ä±ktÄ± videosuna yaz
            video_writer.write(annotated_frame_bgr)

            # Ä°lerlemeyi gÃ¼ncelle
            frame_count += 1
            pbar.update(1)

            # Her 100 framede bir ilerleme raporu ver
            if frame_count % 100 == 0:
                current_frame = start_frame + frame_count
                print(f"Ä°lerleme: {frame_count}/{frames_to_process} frame iÅŸlendi (Frame #{current_frame})")

    # KaynaklarÄ± serbest bÄ±rak
    cap.release()
    video_writer.release()

    print(f"\nâœ… Ä°ÅŸlenmiÅŸ video kaydedildi: {output_video_path}")
    print(f"Toplam {frame_count} frame iÅŸlendi.")

# Fonksiyonu Ã§alÄ±ÅŸtÄ±r
if __name__ == "__main__":
    SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_10_adana-demirspor_ittifak-holding-konyaspor.mp4"
    OUTPUT_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/analysis_output_2000_3000.mp4"

    # 2000. frameden baÅŸlayarak 1000 frame iÅŸle
    process_football_video(SOURCE_VIDEO_PATH, OUTPUT_VIDEO_PATH, start_frame=2000, max_frames=1000)

"""###### AdÄ±m 5: Saha tespiti ve Ã§izimi test edelim"""

try:
    print("Saha tespiti ve Ã§izimi test ediliyor...")

    # Pitch gÃ¶rÃ¼nÃ¼mÃ¼ Ã§iz
    pitch_view = draw_pitch(CONFIG)

    # 3:51 zaman noktasÄ±ndaki frame'i al
    import cv2
    cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)
    fps = cap.get(cv2.CAP_PROP_FPS)
    target_time = 3 * 60 + 51  # 3:51 (dakika:saniye)
    target_frame = int(target_time * fps)

    # Ä°stenen frame'e atla
    cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
    ret, test_frame = cap.read()

    if not ret:
        raise Exception("3:51 zaman noktasÄ±ndaki frame okunamadÄ±")

    # Saha kÃ¶ÅŸe noktalarÄ±nÄ± tespit et
    field_result = FIELD_DETECTION_MODEL.infer(test_frame, confidence=0.3)[0]
    key_points = sv.KeyPoints.from_inference(field_result)

    # GÃ¼venilir kÃ¶ÅŸe noktalarÄ±nÄ± filtrele
    filter = key_points.confidence[0] > 0.5
    print(f"   Tespit edilen gÃ¼venilir kÃ¶ÅŸe noktalarÄ±: {np.sum(filter)}/10")

    if np.sum(filter) >= 4:  # En az 4 kÃ¶ÅŸe noktasÄ± gerekli
        frame_reference_points = key_points.xy[0][filter]
        pitch_reference_points = np.array(CONFIG.vertices)[filter]

        # GÃ¶rÃ¼ÅŸ dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼sÃ¼nÃ¼ oluÅŸtur
        transformer = ViewTransformer(
            source=frame_reference_points,
            target=pitch_reference_points
        )

        # Test iÃ§in bir nokta dÃ¶nÃ¼ÅŸtÃ¼r
        height, width = test_frame.shape[:2]  # Frame boyutlarÄ±nÄ± doÄŸru ÅŸekilde al
        test_point = np.array([[width/2, height/2]])  # EkranÄ±n ortasÄ±
        transformed_point = transformer.transform_points(points=test_point)

        print(f"âœ… Saha tespiti ve dÃ¶nÃ¼ÅŸÃ¼m baÅŸarÄ±lÄ±")
        print(f"   Test noktasÄ± dÃ¶nÃ¼ÅŸÃ¼mÃ¼: Ekran ({width/2}, {height/2}) -> Saha ({transformed_point[0][0]:.1f}, {transformed_point[0][1]:.1f})")

        # GÃ¶rselleÅŸtirme
        plt.figure(figsize=(15, 5))

        # Orijinal frame ve tespit edilen kÃ¶ÅŸe noktalarÄ±
        plt.subplot(1, 3, 1)
        vertex_annotator = sv.VertexAnnotator(color=sv.Color.from_hex('#FF1493'), radius=8)
        frame_reference_key_points = sv.KeyPoints(xy=frame_reference_points[np.newaxis, ...])
        annotated_frame = test_frame.copy()
        annotated_frame = vertex_annotator.annotate(scene=annotated_frame, key_points=frame_reference_key_points)
        plt.imshow(annotated_frame)
        plt.title("Tespit Edilen KÃ¶ÅŸe NoktalarÄ±")
        plt.axis('off')

        # Saha gÃ¶rÃ¼nÃ¼mÃ¼
        plt.subplot(1, 3, 2)
        plt.imshow(pitch_view)
        plt.title("Saha GÃ¶rÃ¼nÃ¼mÃ¼")
        plt.axis('off')

        # DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ saha Ã§izimi
        plt.subplot(1, 3, 3)
        # TÃ¼m pitch kÃ¶ÅŸe noktalarÄ±nÄ± dÃ¶nÃ¼ÅŸtÃ¼r
        pitch_all_points = np.array(CONFIG.vertices)
        try:
            frame_all_points = transformer.transform_points(points=pitch_all_points, source_to_target=False)

            # DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ kÃ¶ÅŸe noktalarÄ±nÄ± Ã§iz
            edge_annotator = sv.EdgeAnnotator(color=sv.Color.from_hex('#00BFFF'), thickness=2, edges=CONFIG.edges)
            frame_all_key_points = sv.KeyPoints(xy=frame_all_points[np.newaxis, ...])
            annotated_frame2 = test_frame.copy()
            annotated_frame2 = edge_annotator.annotate(scene=annotated_frame2, key_points=frame_all_key_points)
            plt.imshow(annotated_frame2)
            plt.title("SahayÄ± Frame'e YansÄ±tma")
        except Exception as e:
            plt.text(0.5, 0.5, f"DÃ¶nÃ¼ÅŸÃ¼m hatasÄ±: {e}", ha='center', va='center')
        plt.axis('off')

        plt.tight_layout()
        plt.show()
    else:
        print("âŒ Yeterli kÃ¶ÅŸe noktasÄ± tespit edilemedi. FarklÄ± bir frame deneyin.")

    # Ä°ÅŸimiz bittiÄŸinde video nesnesini kapat
    cap.release()

except Exception as e:
    print(f"âŒ Saha tespiti test edilirken hata: {e}")

"""###### AdÄ±m 6: TÃ¼m tespit iÅŸlemlerini ve saha yansÄ±tmayÄ± tek bir frame'de test edelim"""

#source pathleri??

try:
    print("Tam iÅŸ akÄ±ÅŸÄ± testi yapÄ±lÄ±yor...")

    # Tracker oluÅŸtur ve sÄ±fÄ±rla
    tracker = sv.ByteTrack()
    tracker.reset()

    # 3:51 zaman noktasÄ±ndaki frame'i al
    import cv2
    cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)
    fps = cap.get(cv2.CAP_PROP_FPS)
    target_time = 3 * 60 + 51  # 3:51 (dakika:saniye)
    target_frame = int(target_time * fps)

    # Ä°stenen frame'e atla
    cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
    ret, test_frame = cap.read()

    if not ret:
        raise Exception("3:51 zaman noktasÄ±ndaki frame okunamadÄ±")

    # BGR'den RGB'ye Ã§evir (gÃ¶rselleÅŸtirme iÃ§in)
    test_frame_rgb = cv2.cvtColor(test_frame, cv2.COLOR_BGR2RGB)

    # 1. Oyuncu, kaleci, top ve hakem tespiti
    result = PLAYER_DETECTION_MODEL.infer(test_frame_rgb, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)

    # Top tespitlerini ayÄ±r
    ball_detections = detections[detections.class_id == BALL_ID]
    if len(ball_detections) > 0:
        ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

    # DiÄŸer tespitleri iÅŸle
    all_detections = detections[detections.class_id != BALL_ID]
    if len(all_detections) > 0:
        all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
        all_detections = tracker.update_with_detections(detections=all_detections)

    # Oyuncu, kaleci ve hakemleri ayÄ±r
    goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]
    players_detections = all_detections[all_detections.class_id == PLAYER_ID]
    referees_detections = all_detections[all_detections.class_id == REFEREE_ID]

    # 2. TakÄ±m tanÄ±mlama (TeamClassifier kullanarak)
    if len(players_detections) > 0:
        # Sports reposundan TeamClassifier'Ä± iÃ§e aktar
        from sports.common.team import TeamClassifier

        # Ã–nce eÄŸitim verileri topla
        print("EÄŸitim verileri iÃ§in daha fazla oyuncu Ã¶rneÄŸi toplanÄ±yor...")
        training_crops = []
        # Videonun farklÄ± bÃ¶lÃ¼mlerinden Ã¶rnekler toplayalÄ±m
        temp_frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, stride=30)
        for i, frame in enumerate(temp_frame_generator):
            if i > 30:  # 30 farklÄ± frame'den veri topla
                break
            temp_result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
            temp_detections = sv.Detections.from_inference(temp_result)
            temp_players = temp_detections[temp_detections.class_id == PLAYER_ID]
            temp_crops = [sv.crop_image(frame, xyxy) for xyxy in temp_players.xyxy]
            training_crops.extend(temp_crops)

        # Mevcut frame'deki oyuncularÄ± da ekle
        players_crops = [sv.crop_image(test_frame_rgb, xyxy) for xyxy in players_detections.xyxy]
        all_crops = training_crops + players_crops

        if len(all_crops) > 0:
            print(f"TeamClassifier, {len(all_crops)} oyuncu gÃ¶rÃ¼ntÃ¼sÃ¼ ile eÄŸitiliyor...")
            team_classifier = TeamClassifier(device="cpu")  # GPU varsa "cuda" olabilir
            team_classifier.fit(all_crops)

            # OyuncularÄ± sÄ±nÄ±flandÄ±r
            players_detections.class_id = team_classifier.predict(players_crops)
            print(f"Oyuncular {np.sum(players_detections.class_id == 0)} - {np.sum(players_detections.class_id == 1)} olarak sÄ±nÄ±flandÄ±rÄ±ldÄ±.")

    # Kaleci takÄ±m atamasÄ±
    if len(goalkeepers_detections) > 0 and len(players_detections) > 0:
        # Kaleci takÄ±m atama fonksiyonu
        def resolve_goalkeepers_team_id(players_detections, goalkeepers_detections):
            """Kalecilerin takÄ±mlarÄ±nÄ±, en yakÄ±n oyuncularÄ±n takÄ±mlarÄ±nÄ± referans alarak belirler."""
            if len(goalkeepers_detections) == 0 or len(players_detections) == 0:
                return np.array([])

            gk_team_ids = []

            for gk_idx in range(len(goalkeepers_detections)):
                gk_xy = goalkeepers_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)[gk_idx]

                player_distances = []
                for player_idx in range(len(players_detections)):
                    player_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)[player_idx]
                    distance = np.sqrt(np.sum((gk_xy - player_xy) ** 2))
                    player_distances.append((distance, players_detections.class_id[player_idx]))

                # En yakÄ±n 3 oyuncuyu al
                player_distances.sort(key=lambda x: x[0])
                closest_teams = [team_id for _, team_id in player_distances[:3]]

                # En Ã§ok hangi takÄ±m yakÄ±nsa o takÄ±mda
                if len(closest_teams) > 0:
                    most_common_team = max(set(closest_teams), key=closest_teams.count)
                    gk_team_ids.append(most_common_team)
                else:
                    gk_team_ids.append(0)  # VarsayÄ±lan olarak 0. takÄ±m

            return np.array(gk_team_ids)

        goalkeepers_detections.class_id = resolve_goalkeepers_team_id(
            players_detections, goalkeepers_detections)
        print(f"Kaleciler takÄ±mlara atandÄ±: {goalkeepers_detections.class_id}")

    if len(referees_detections) > 0:
        # Hakemleri -1 olarak iÅŸaretle (farklÄ± renkle gÃ¶stermek iÃ§in)
        referees_detections.class_id = np.zeros(len(referees_detections), dtype=int) - 1

    # 3. Saha kÃ¶ÅŸe noktalarÄ±nÄ± tespit et
    try:
        field_result = FIELD_DETECTION_MODEL.infer(test_frame_rgb, confidence=0.3)[0]
        key_points = sv.KeyPoints.from_inference(field_result)

        # GÃ¼venilir kÃ¶ÅŸe noktalarÄ±nÄ± filtrele
        filter = key_points.confidence[0] > 0.5
        print(f"Tespit edilen kÃ¶ÅŸe noktalarÄ±: {np.sum(filter)}/10")
    except Exception as e:
        print(f"Saha tespitinde hata: {e}")
        filter = np.array([False] * 10)  # VarsayÄ±lan filtre (hiÃ§bir nokta gÃ¼venilir deÄŸil)
        key_points = None

    # Yeterli kÃ¶ÅŸe noktasÄ± var mÄ± kontrol et
    if np.sum(filter) >= 4:  # En az 4 kÃ¶ÅŸe noktasÄ± gerekli
        frame_reference_points = key_points.xy[0][filter]
        pitch_reference_points = np.array(CONFIG.vertices)[filter]

        # GÃ¶rÃ¼ÅŸ dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼sÃ¼nÃ¼ oluÅŸtur
        transformer = ViewTransformer(
            source=frame_reference_points,
            target=pitch_reference_points
        )

        # 4. BirleÅŸik tespitleri oluÅŸtur
        all_game_detections = sv.Detections.merge([players_detections, goalkeepers_detections])

        # 5. GÃ¶rselleÅŸtirme - Ana gÃ¶rseller
        plt.figure(figsize=(18, 10))

        # Orijinal frame ve tespitler
        plt.subplot(1, 2, 1)
        ellipse_annotator = sv.EllipseAnnotator(
            color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']), thickness=2)
        triangle_annotator = sv.TriangleAnnotator(
            color=sv.Color.from_hex('#FFD700'), base=20, height=17)

        annotated_frame = test_frame_rgb.copy()
        if len(all_game_detections) > 0:
            annotated_frame = ellipse_annotator.annotate(scene=annotated_frame, detections=all_game_detections)
        if len(ball_detections) > 0:
            annotated_frame = triangle_annotator.annotate(scene=annotated_frame, detections=ball_detections)
        if len(referees_detections) > 0:
            # Hakemleri sarÄ± renkle iÅŸaretle
            referee_annotator = sv.EllipseAnnotator(
                color=sv.ColorPalette.from_hex(['#FFD700']), thickness=2)
            annotated_frame = referee_annotator.annotate(scene=annotated_frame, detections=referees_detections)

        plt.imshow(annotated_frame)
        plt.title("Tespit Edilen Nesneler")
        plt.axis('off')

        # Saha gÃ¶rÃ¼nÃ¼mÃ¼
        plt.subplot(1, 2, 2)
        pitch_view = draw_pitch(CONFIG)

        # Top varsa yansÄ±t
        if len(ball_detections) > 0:
            frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
            pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)

            # ToplarÄ± Ã§iz
            pitch_view = draw_points_on_pitch(
                config=CONFIG,
                xy=pitch_ball_xy,
                face_color=sv.Color.from_hex('#FF00FF'),  # Mor
                edge_color=sv.Color.BLACK,
                radius=10,
                pitch=pitch_view
            )

        # OyuncularÄ± yansÄ±t
        if len(all_game_detections) > 0:
            players_xy = all_game_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
            pitch_players_xy = transformer.transform_points(points=players_xy)

            # TakÄ±m 1 oyuncularÄ±
            team1_mask = all_game_detections.class_id == 0
            if np.any(team1_mask):
                pitch_view = draw_points_on_pitch(
                    config=CONFIG,
                    xy=pitch_players_xy[team1_mask],
                    face_color=sv.Color.from_hex('#FFFF00'),  # SarÄ±
                    edge_color=sv.Color.BLACK,
                    radius=16,
                    pitch=pitch_view
                )

            # TakÄ±m 2 oyuncularÄ±
            team2_mask = all_game_detections.class_id == 1
            if np.any(team2_mask):
                pitch_view = draw_points_on_pitch(
                    config=CONFIG,
                    xy=pitch_players_xy[team2_mask],
                    face_color=sv.Color.from_hex('#00BFFF'),  # Mavi
                    edge_color=sv.Color.BLACK,
                    radius=16,
                    pitch=pitch_view
                )

        # Hakemleri yansÄ±t
        if len(referees_detections) > 0:
            referees_xy = referees_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
            pitch_referees_xy = transformer.transform_points(points=referees_xy)

            pitch_view = draw_points_on_pitch(
                config=CONFIG,
                xy=pitch_referees_xy,
                face_color=sv.Color.from_hex('#FF69B4'),  # Pembe
                edge_color=sv.Color.BLACK,
                radius=16,
                pitch=pitch_view
            )

        plt.imshow(pitch_view)
        plt.title("Saha Ãœzerinde Oyuncular ve Top")
        plt.axis('off')

        plt.tight_layout()
        plt.show()

        # 6. Ä°kinci figÃ¼r - TakÄ±m Ã¶rnekleri ve bilgiler
        if len(players_crops) > 0:
            plt.figure(figsize=(12, 8))

            # Sol panel: TakÄ±m Ã¶rnekleri
            plt.subplot(1, 2, 1)
            team1_crops = [crop for i, crop in enumerate(players_crops) if players_detections.class_id[i] == 0]
            team2_crops = [crop for i, crop in enumerate(players_crops) if players_detections.class_id[i] == 1]

            plt.axis('off')
            plt.title("TakÄ±m Ã–rnekleri", fontsize=14)

            # TakÄ±m 1 Ã¶rnekleri
            if len(team1_crops) > 0:
                plt.figure(figsize=(6, 4))
                plt.suptitle("TakÄ±m 1", fontsize=16)
                for i, crop in enumerate(team1_crops[:3]):  # Sadece ilk 3'Ã¼ gÃ¶ster
                    plt.subplot(1, 3, i+1)
                    plt.imshow(crop)
                    plt.axis('off')
                plt.tight_layout()
                plt.show()

            # TakÄ±m 2 Ã¶rnekleri
            if len(team2_crops) > 0:
                plt.figure(figsize=(6, 4))
                plt.suptitle("TakÄ±m 2", fontsize=16)
                for i, crop in enumerate(team2_crops[:3]):  # Sadece ilk 3'Ã¼ gÃ¶ster
                    plt.subplot(1, 3, i+1)
                    plt.imshow(crop)
                    plt.axis('off')
                plt.tight_layout()
                plt.show()

            # Ä°statistikler
            plt.figure(figsize=(8, 6))
            plt.axis('off')
            info_text = f"""
            Frame bilgileri:

            - Top sayÄ±sÄ±: {len(ball_detections)}
            - Oyuncu sayÄ±sÄ±: {len(players_detections)}
              * TakÄ±m 1 (SarÄ±): {np.sum(players_detections.class_id == 0)} oyuncu
              * TakÄ±m 2 (Mavi): {np.sum(players_detections.class_id == 1)} oyuncu
            - Kaleci sayÄ±sÄ±: {len(goalkeepers_detections)}
              * TakÄ±m 1 (SarÄ±): {np.sum(goalkeepers_detections.class_id == 0)} kaleci
              * TakÄ±m 2 (Mavi): {np.sum(goalkeepers_detections.class_id == 1)} kaleci
            - Hakem sayÄ±sÄ±: {len(referees_detections)}
            - Tespit edilen kÃ¶ÅŸe noktalarÄ±: {np.sum(filter)}/10
            """
            plt.text(0.1, 0.5, info_text, va='center', fontsize=12)
            plt.title("Ä°statistikler", fontsize=14)
            plt.tight_layout()
            plt.show()

        # Ä°ÅŸimiz bittiÄŸinde video nesnesini kapat
        cap.release()

        print("âœ… Tam iÅŸ akÄ±ÅŸÄ± testi baÅŸarÄ±lÄ±!")
    else:
        print("âŒ Yeterli kÃ¶ÅŸe noktasÄ± tespit edilemedi. FarklÄ± bir frame deneyin.")
        cap.release()
except Exception as e:
    import traceback
    print(f"âŒ Tam iÅŸ akÄ±ÅŸÄ± testi sÄ±rasÄ±nda hata: {e}")
    print(traceback.format_exc())  # Tam hata izini yazdÄ±r
    if 'cap' in locals():
        cap.release()

"""###### AdÄ±m 7: Video oluÅŸturma 2D"""

import os
import numpy as np
import supervision as sv
import cv2
import matplotlib.pyplot as plt
from tqdm import tqdm
from sports.common.team import TeamClassifier
from sports.common.view import ViewTransformer
from sports.annotators.soccer import draw_pitch, draw_points_on_pitch
from sports.configs.soccer import SoccerPitchConfiguration

# 4000-6000 frame aralÄ±ÄŸÄ± iÃ§in parametreler
def process_soccer_video(
    source_video_path=SOURCE_VIDEO_PATH,
    output_video_path=OUTPUT_VIDEO_PATH,
    start_frame=4000,    # 4000. frame'den baÅŸla
    max_frames=2000,     # 2000 frame iÅŸle (4000'den baÅŸlayÄ±p 6000'e kadar)
    confidence_threshold=0.3,
    training_stride=30,  # EÄŸitim iÃ§in her 30 frame'de bir Ã¶rnek al
    training_frames=50   # 50 frame'den veri topla
):
    """
    Futbol videosunu iÅŸleyerek oyuncularÄ±, kalecileri ve hakemler tespit eder,
    takÄ±m sÄ±nÄ±flandÄ±rmasÄ± yapar ve sonuÃ§larÄ± bir video olarak kaydeder.

    Args:
        source_video_path: Kaynak video dosyasÄ±nÄ±n yolu
        output_video_path: Ã‡Ä±ktÄ± video dosyasÄ±nÄ±n yolu
        start_frame: BaÅŸlangÄ±Ã§ frame numarasÄ±
        max_frames: Ä°ÅŸlenecek maksimum frame sayÄ±sÄ± (None ise tÃ¼m video)
        confidence_threshold: Nesne tespiti iÃ§in minimum gÃ¼ven eÅŸiÄŸi
        training_stride: EÄŸitim verileri toplarken atlanan frame sayÄ±sÄ±
        training_frames: EÄŸitim iÃ§in kullanÄ±lacak frame sayÄ±sÄ±
    """

    print("Futbol maÃ§Ä± analizi baÅŸlatÄ±lÄ±yor...")

    # Nesne ID'leri
    BALL_ID = 0
    GOALKEEPER_ID = 1
    PLAYER_ID = 2
    REFEREE_ID = 3

    # Saha konfigÃ¼rasyonu
    CONFIG = SoccerPitchConfiguration()

    # Video bilgilerini al
    print("Video bilgileri alÄ±nÄ±yor...")
    cap = cv2.VideoCapture(source_video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Ä°ÅŸlenecek frame sayÄ±sÄ±nÄ± belirle
    if max_frames is None:
        max_frames = total_frames - start_frame
    end_frame = min(start_frame + max_frames, total_frames)
    frames_to_process = end_frame - start_frame

    print(f"Video Ã¶zellikleri: {width}x{height}, {fps} FPS, {total_frames} toplam frame")
    print(f"Ä°ÅŸlenecek frame aralÄ±ÄŸÄ±: {start_frame} - {end_frame-1} ({frames_to_process} frame)")

    # Ã‡Ä±ktÄ± dizinini oluÅŸtur
    output_dir = os.path.dirname(output_video_path)
    if not os.path.exists(output_dir) and output_dir:
        os.makedirs(output_dir)

    # EÄŸitim verileri toplama
    print("\n1. TakÄ±m sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± iÃ§in eÄŸitim verileri toplanÄ±yor...")
    training_crops = []

    # EÄŸitim verileri iÃ§in frame jeneratÃ¶rÃ¼
    training_frame_generator = sv.get_video_frames_generator(source_video_path, stride=training_stride)

    for i, frame in enumerate(tqdm(training_frame_generator, desc="EÄŸitim verileri toplama")):
        if i >= training_frames:
            break

        # Nesneleri tespit et
        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=confidence_threshold)[0]
        detections = sv.Detections.from_inference(result)
        players = detections[detections.class_id == PLAYER_ID]

        # Oyuncu gÃ¶rÃ¼ntÃ¼lerini topla
        if len(players) > 0:
            player_crops = [sv.crop_image(frame, xyxy) for xyxy in players.xyxy]
            training_crops.extend(player_crops)

    if len(training_crops) == 0:
        print("âš ï¸ HiÃ§ eÄŸitim verisi toplanamadÄ±! LÃ¼tfen video iÃ§eriÄŸini kontrol edin.")
        return

    print(f"âœ… {len(training_crops)} oyuncu gÃ¶rÃ¼ntÃ¼sÃ¼ toplandÄ±.")

    # TeamClassifier'Ä± eÄŸit
    print("\n2. TeamClassifier eÄŸitiliyor...")
    team_classifier = TeamClassifier(device="cpu")  # GPU varsa "cuda" kullanabilirsiniz
    team_classifier.fit(training_crops)
    print("âœ… TeamClassifier baÅŸarÄ±yla eÄŸitildi!")

    # Kaleci takÄ±m atamasÄ± iÃ§in fonksiyon
    def resolve_goalkeepers_team_id(players_detections, goalkeepers_detections):
        """Kalecilerin takÄ±mlarÄ±nÄ±, en yakÄ±n oyuncularÄ±n takÄ±mlarÄ±nÄ± referans alarak belirler."""
        if len(goalkeepers_detections) == 0 or len(players_detections) == 0:
            return np.array([])

        gk_team_ids = []

        for gk_idx in range(len(goalkeepers_detections)):
            gk_xy = goalkeepers_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)[gk_idx]

            player_distances = []
            for player_idx in range(len(players_detections)):
                player_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)[player_idx]
                distance = np.sqrt(np.sum((gk_xy - player_xy) ** 2))
                player_distances.append((distance, players_detections.class_id[player_idx]))

            # En yakÄ±n 3 oyuncuyu al
            player_distances.sort(key=lambda x: x[0])

            # DÃ¼zeltilmiÅŸ kod
            if len(player_distances) >= 3:
                closest_teams = [team_id for _, team_id in player_distances[:3]]
            else:
                closest_teams = [team_id for _, team_id in player_distances]

            # En Ã§ok hangi takÄ±m yakÄ±nsa o takÄ±mda
            if len(closest_teams) > 0:
                most_common_team = max(set(closest_teams), key=closest_teams.count)
                gk_team_ids.append(most_common_team)
            else:
                gk_team_ids.append(0)  # VarsayÄ±lan olarak 0. takÄ±m

        return np.array(gk_team_ids)

    # Tracker baÅŸlat
    tracker = sv.ByteTrack()
    tracker.reset()

    # GÃ¶rselleÅŸtirme iÃ§in annotatorler
    ellipse_annotator = sv.EllipseAnnotator(
        color=sv.ColorPalette.from_hex(['#FFFF00', '#00BFFF', '#FF69B4']),  # SarÄ±, Mavi, Pembe
        thickness=2
    )
    triangle_annotator = sv.TriangleAnnotator(
        color=sv.Color.from_hex('#FF00FF'),  # Mor
        base=20,
        height=17
    )

    # Ã‡Ä±ktÄ± video yazÄ±cÄ±sÄ±nÄ± oluÅŸtur
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(
        output_video_path,
        fourcc,
        fps,
        (width*2, height)  # Yan yana iki gÃ¶rÃ¼ntÃ¼
    )

    # Ä°stenen baÅŸlangÄ±Ã§ frame'ine git
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

    # TÃ¼m videoyu iÅŸle
    print(f"\n3. Video iÅŸleniyor ve {output_video_path} yoluna kaydediliyor...")

    with tqdm(total=frames_to_process, desc="Video iÅŸleme") as pbar:
        frame_count = 0

        while cap.isOpened() and frame_count < frames_to_process:
            ret, frame = cap.read()
            if not ret:
                break

            # BGR -> RGB dÃ¶nÃ¼ÅŸÃ¼mÃ¼
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # 1. Nesneleri tespit et
            result = PLAYER_DETECTION_MODEL.infer(frame_rgb, confidence=confidence_threshold)[0]
            detections = sv.Detections.from_inference(result)

            # Top tespitlerini ayÄ±r
            ball_detections = detections[detections.class_id == BALL_ID]
            if len(ball_detections) > 0:
                ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

            # DiÄŸer nesneleri iÅŸle
            all_detections = detections[detections.class_id != BALL_ID]
            if len(all_detections) > 0:
                all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)
                all_detections = tracker.update_with_detections(detections=all_detections)

            # Oyuncu, kaleci ve hakemleri ayÄ±r
            goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]
            players_detections = all_detections[all_detections.class_id == PLAYER_ID]
            referees_detections = all_detections[all_detections.class_id == REFEREE_ID]

            # 2. TakÄ±m sÄ±nÄ±flandÄ±rmasÄ±
            if len(players_detections) > 0:
                players_crops = [sv.crop_image(frame_rgb, xyxy) for xyxy in players_detections.xyxy]
                players_detections.class_id = team_classifier.predict(players_crops)

            # 3. Kaleci takÄ±m atamasÄ±
            if len(goalkeepers_detections) > 0 and len(players_detections) > 0:
                goalkeepers_detections.class_id = resolve_goalkeepers_team_id(
                    players_detections, goalkeepers_detections)

            # 4. Hakemleri -1 olarak iÅŸaretle
            if len(referees_detections) > 0:
                referees_detections.class_id = np.full(len(referees_detections), 2)  # 2 = hakem iÃ§in

            # 5. Saha kÃ¶ÅŸe noktalarÄ±nÄ± tespit et
            try:
                field_result = FIELD_DETECTION_MODEL.infer(frame_rgb, confidence=confidence_threshold)[0]
                key_points = sv.KeyPoints.from_inference(field_result)
                filter = key_points.confidence[0] > 0.5
            except Exception as e:
                if frame_count == 0:  # Sadece ilk hatayÄ± gÃ¶ster
                    print(f"Saha tespitinde hata: {e}")
                filter = np.array([False] * 10)

            # 6. GÃ¶rselleÅŸtirme
            annotated_frame = frame_rgb.copy()

            # TÃ¼m oyuncularÄ± ve kalecileri birleÅŸtir
            all_game_detections = sv.Detections.merge([players_detections, goalkeepers_detections])

            # AnotasyonlarÄ± uygula
            if len(all_game_detections) > 0:
                annotated_frame = ellipse_annotator.annotate(scene=annotated_frame, detections=all_game_detections)
            if len(ball_detections) > 0:
                annotated_frame = triangle_annotator.annotate(scene=annotated_frame, detections=ball_detections)
            if len(referees_detections) > 0:
                # Hakem anotasyonu
                referee_annotator = sv.EllipseAnnotator(color=sv.ColorPalette.from_hex(['#FF69B4']), thickness=2)
                annotated_frame = referee_annotator.annotate(scene=annotated_frame, detections=referees_detections)

            # Saha gÃ¶rÃ¼ntÃ¼sÃ¼ oluÅŸtur
            pitch_view = draw_pitch(CONFIG)

            # EÄŸer yeterli kÃ¶ÅŸe noktasÄ± varsa, oyuncularÄ± ve topu saha Ã¼zerine yansÄ±t
            if np.sum(filter) >= 4:
                frame_reference_points = key_points.xy[0][filter]
                pitch_reference_points = np.array(CONFIG.vertices)[filter]

                transformer = ViewTransformer(
                    source=frame_reference_points,
                    target=pitch_reference_points
                )

                # Top varsa saha Ã¼zerine yansÄ±t
                if len(ball_detections) > 0:
                    frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
                    pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)

                    pitch_view = draw_points_on_pitch(
                        config=CONFIG,
                        xy=pitch_ball_xy,
                        face_color=sv.Color.from_hex('#FF00FF'),  # Mor
                        edge_color=sv.Color.BLACK,
                        radius=10,
                        pitch=pitch_view
                    )

                # OyuncularÄ± saha Ã¼zerine yansÄ±t
                if len(all_game_detections) > 0:
                    players_xy = all_game_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
                    pitch_players_xy = transformer.transform_points(points=players_xy)

                    # TakÄ±m 1 (SarÄ±)
                    team1_mask = all_game_detections.class_id == 0
                    if np.any(team1_mask):
                        pitch_view = draw_points_on_pitch(
                            config=CONFIG,
                            xy=pitch_players_xy[team1_mask],
                            face_color=sv.Color.from_hex('#FFFF00'),  # SarÄ±
                            edge_color=sv.Color.BLACK,
                            radius=16,
                            pitch=pitch_view
                        )

                    # TakÄ±m 2 (Mavi)
                    team2_mask = all_game_detections.class_id == 1
                    if np.any(team2_mask):
                        pitch_view = draw_points_on_pitch(
                            config=CONFIG,
                            xy=pitch_players_xy[team2_mask],
                            face_color=sv.Color.from_hex('#00BFFF'),  # Mavi
                            edge_color=sv.Color.BLACK,
                            radius=16,
                            pitch=pitch_view
                        )

                # Hakemleri saha Ã¼zerine yansÄ±t
                if len(referees_detections) > 0:
                    referees_xy = referees_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
                    pitch_referees_xy = transformer.transform_points(points=referees_xy)

                    pitch_view = draw_points_on_pitch(
                        config=CONFIG,
                        xy=pitch_referees_xy,
                        face_color=sv.Color.from_hex('#FF69B4'),  # Pembe
                        edge_color=sv.Color.BLACK,
                        radius=16,
                        pitch=pitch_view
                    )

            # Ä°statistikleri ekle
            team1_count = np.sum(players_detections.class_id == 0) if len(players_detections) > 0 else 0
            team2_count = np.sum(players_detections.class_id == 1) if len(players_detections) > 0 else 0

            cv2.putText(
                annotated_frame,
                f"TakÄ±m 1 (SarÄ±): {team1_count}, TakÄ±m 2 (Mavi): {team2_count}, Top: {len(ball_detections)}, Hakem: {len(referees_detections)}",
                (10, 25),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2
            )

            # KÃ¶ÅŸe noktasÄ± sayÄ±sÄ±nÄ± ekle
            cv2.putText(
                annotated_frame,
                f"KÃ¶ÅŸe noktalarÄ±: {np.sum(filter)}/10",
                (10, 50),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2
            )

            # Frame numarasÄ±nÄ± ekle
            current_frame = start_frame + frame_count
            cv2.putText(
                annotated_frame,
                f"Frame: {current_frame}/{total_frames}",
                (10, height - 20),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2
            )

            # GÃ¶rÃ¼ntÃ¼leri yan yana birleÅŸtir
            # pitch_view'Ä± annotated_frame ile aynÄ± boyuta getir
            pitch_view_resized = cv2.resize(pitch_view, (width, height))

            # RGB'den BGR'ye Ã§evir (OpenCV iÃ§in)
            annotated_frame_bgr = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)
            pitch_view_bgr = cv2.cvtColor(pitch_view_resized, cv2.COLOR_RGB2BGR)

            # Ä°ki gÃ¶rÃ¼ntÃ¼yÃ¼ yan yana birleÅŸtir
            combined_frame = np.hstack((annotated_frame_bgr, pitch_view_bgr))

            # Videoyu kaydet
            out.write(combined_frame)

            # Ä°lerlemeyi gÃ¼ncelle
            frame_count += 1
            pbar.update(1)

    # KaynaklarÄ± serbest bÄ±rak
    cap.release()
    out.release()

    print(f"\nâœ… Ä°ÅŸlem tamamlandÄ±! Ã‡Ä±ktÄ± videosu: {output_video_path}")
    print(f"Toplam {frame_count} frame iÅŸlendi.")

# Kodu Ã§alÄ±ÅŸtÄ±r
if __name__ == "__main__":
    SOURCE_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/downloads/2023-2024/2023-2024_11_fraport-tav-antalyaspor_besiktas.mp4"
    OUTPUT_VIDEO_PATH = "/content/drive/MyDrive/videos_bein_sports/betul/analysis_team_classifier_4000frames.mp4"

    # Ä°lk 4000 frame iÃ§in parametreler
    process_soccer_video(
        source_video_path=SOURCE_VIDEO_PATH,
        output_video_path=OUTPUT_VIDEO_PATH,
        start_frame=0,      # 0. frame'den baÅŸla
        max_frames=4000,    # Ä°lk 4000 frame'i iÅŸle MAÃ‡ TAMAMI Ä°Ã‡Ä°N BU KISIM DEÄžÄ°ÅžECEK
        confidence_threshold=0.3,
        training_stride=30, # EÄŸitim iÃ§in her 30 frame'de bir Ã¶rnek al
        training_frames=50  # 50 frame'den veri topla
    )